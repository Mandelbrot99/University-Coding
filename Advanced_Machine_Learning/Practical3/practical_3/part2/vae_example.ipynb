{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic VAE implementation\n",
    "\n",
    "This notebook defines a basic VAE to model the MNIST dataset, using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Theory\n",
    "\n",
    "The goal of a VAE is to (implicitly) learn a distribution $p(x),\\,x \\in X$, with $X$ a vector space, which is as similar as possible to an unknown target distribution $p^{*}(x)$, given a set of samples from this distribution $\\mathcal{D}=\\{x_1, ..., x_{n}\\}$. VAEs make the assumption that $x$ depends on some set of underlying latent variables $z \\in Z$, which are distributed according to some known distribution $p(z)$, and model the likelihood $p(x\\mid z)$ and an approximation of the posterior distribution $p(z \\mid x)$.\n",
    "\n",
    "To help them learn, VAEs use variational inference. In this approach, instead of computing the posterior $p(z \\mid x)$, which is often intractable, one searches for a more tractable distribution $q(z \\mid x)$ which approximates the posterior. VAEs assume parameterised models of the likelihood and the approximate posterior, given by $p(x \\mid z,\\theta)$ and $q(z \\mid x, \\theta)$, where $\\theta$ represent model parameters. To learn  $q(z \\mid x, \\theta)$, we maximise the Evidence Lower Bound (or ELBO) over the training data, which is given by\n",
    "\n",
    "\\begin{equation}\n",
    "ELBO = \\sum_{i=1}^{n} \\mathbb{E}_{q(z_{i} \\mid x_{i},\\theta)} \\left[\\log p(x_{i} \\mid z_{i},\\theta) \\right] - D_{\\mathrm{KL}}(q(z_{i}\\mid x_{i},\\theta) \\, \\| \\, p(z_{i}))~,\n",
    "\\end{equation}\n",
    "assuming the training samples are independently and identically distributed. \n",
    "\n",
    "To be able to compute the ELBO, we assume a Gaussian VAE:\n",
    "\\begin{align}\n",
    "\\label{eq:vae_likelihood}\n",
    "p(x \\mid z,\\theta)&=\\mathcal{N}(x; f(z; \\theta_{f}), \\sigma^{2} I)\n",
    "\\\\\n",
    "\\label{eq:vae_prior}\n",
    "p(z)&=\\mathcal{N}(z; 0, I)\n",
    "\\\\\n",
    "\\label{eq:vae_q}\n",
    "q(z\\mid x,\\theta)&=\\mathcal{N}(z; g(x;\\theta_{g}), \\mathrm{diag}(h(x;\\theta_{h})^{2}))~,\n",
    "\\end{align}\n",
    "where $f$, $g$ and $h$ are all deep neural networks parameterised by $\\theta=\\{\\theta_{f},\\theta_{g},\\theta_{h}\\}$, $\\sigma$ is a hyperparameter. Our optimal network parameters are then given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:vae_mle}\n",
    "\\hat \\theta_{f}, \\hat \\theta_{g}, \\hat \\theta_{h} = argmin_{\\theta_{f}, \\theta_{g}, \\theta_{h}}  \\left\\{ \\sum_{i=1}^{n} \\mathbb{E}_{q(z_{i} \\mid x_{i}, \\theta_{g},\\theta_{h})} \\left[{\\| x_{i}-f(z_{i};\\theta_{f}) \\|^{2} \\over 2 \\sigma^{2}}\\right] + D_{\\mathrm{KL}}(q(z_{i}\\mid x_{i},\\theta_{g},\\theta_{h}) \\, \\| \\, p(z_{i})) \\right\\}~.\n",
    "\\end{equation}\n",
    "\n",
    "The KL term above can easily be computed analytically as it involves two normal distributions. When calculating the expectation over $q(z\\mid x,\\theta)$, which is usually carried out via sampling, a \"reparameterisation trick\" is used. One notes that sampling $z \\sim q(z\\mid x,\\theta)$ is equivalent to sampling $\\epsilon \\sim \\mathcal{N}(\\epsilon;0,I)$ and computing $z=h(x;\\theta_{h})\\epsilon + g(x;\\theta_{g})$, which allows gradient descent methods to be used to find $\\hat \\theta$.\n",
    "\n",
    "> **Extension task**: show that the KL divergence term in this case can be analytically derived as: $D_{\\mathrm{KL}}(q(z_{i}\\mid x_{i},\\theta_{g},\\theta_{h}) \\, \\| \\, p(z_{i}))= -{1\\over 2} \\sum^{J}_{j=1} \\left[ 1 + \\log(h_{j}^{2}) - h_{j}^{2} - g_{j}^{2} \\right]$ where $J$ is the dimension of the latent vector.\n",
    "\n",
    "For more on VAE theory, see: https://arxiv.org/abs/1606.05908 and https://arxiv.org/abs/1907.08956."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data\n",
    "\n",
    "First, we load the MNIST training data (similar to before), and plot some training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571bb6bd8e674c4281e174edaa6746d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c79976ef71430d9462362d02fb7f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dabf0e906ff5451e95d19c673dfe7ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944a9811df874a8aa51bbab4d08cd9e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/anaconda3/envs/aml-practical/lib/python3.8/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370172916/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                   ]))\n",
    "\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                   ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS4AAAEICAYAAADhtRloAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAXeElEQVR4nO3df5BdZX3H8fenm0AaiEIawRSipBhE1Bp1C3SwGGvBwNgC06IER5FqI0ja2tIWynQKbXWKrdIiIpkFU2AGwV9Yoo1SZaqgBUygARJiMMYUlsTEEPmpkOzdb/84N+Xu3Xufe3b33j3nbD6vmTN7z/2eH9+5wJfzPOc5z1FEYGZWJb9UdAJmZmPlwmVmlePCZWaV48JlZpXjwmVmlePCZWaV48JlZpXjwmUtSfq2pOclPVtfNhadk9leLlyWsiwiDqwvry46GbO9XLjMrHJcuCzlHyXtlPQ9SYuKTsZsL/lZRWtF0nHAw8Bu4Czg08DCiPhRoYmZ4cJlOUn6BvAfEXFV0bmYualoeQWgopMwAxcua0HSQZLeIWmGpGmS3gOcCNxedG5mANOKTsBKaTrwUeBooAb8ADg9IjyWy0rBfVxmVjluKppZ5bhwmVnPSFohaYekdW3ikvQpSZskPSjpTXmO68JlZr10PbA4ET8FWFBflgLX5DmoC5eZ9UxE3AnsSmxyGnBjZO4BDpI0t9NxJ/Wu4n7aP2ZwwGSe0myf8jzPsTtemNB4u3e87YB4Ylct17b3PfjCeuD5hq8GImJgDKc7DHisYX2w/t221E4TKlySFgNXAn3AdRFxeWr7GRzAcXr7RE5pZgn3xh0TPsYTu2p8//ZX5Nq2b+4Pn4+I/gmcrlWR7TjUYdyFS1IfcDVwElmVXC1pZUQ8PN5jmlnxAhhmeLJONwjMa1g/HNjaaaeJ9HEdC2yKiM0RsRu4hay9amYVFgR7opZr6YKVwPvqdxePB56KiGQzESbWVGzVNj2ueSNJS8nuFjCDmRM4nZlNlm5dcUm6GVgEzJE0CFxK9mQGEbEcWAWcCmwCfg6cm+e4Eylcudqm9Y66AYCXaLaH6ZuVXBDUuvRETUQs6RAP4IKxHncihWtcbVMzK7/hzv3jhZpI4VoNLJA0H3icbLK5s7uSlZkVJoDaVC1cETEkaRnZVCd9wIqIWN+1zMysMFP5iouIWEXWuWZmU0QAe0o+a4zn4zKzEYKYuk1FM5uiAmrlrlsuXGY2UjZyvtxcuMysiaiV/L0oLlxmNkLWOe/CZWYVko3jcuEys4oZ9hWXmVWJr7jMrHICUSv5rO4uXGY2ipuKZlYpgdgdfUWnkeTCZWYjZANQ3VQ0s4px57yZVUqEqIWvuMysYoZ9xWVmVZJ1zpe7NJQ7OzObdO6cN7NKqnkcl5lViUfOm1klDfuuoplVSfaQtQuXmVVIIPb4kR8zq5IIPADVzKpGHoBqZtUS+IrLzCrInfNmVimBPJGgmVVL9nqycpeGcmdnZgXwC2GtYJqW/kfc97I5PT3/xr84om2sNjP9ovdXHrkjGZ/54fR/XD+5Yr+2sfv7P5/cd2ftuWT8uC9emIy/6s/vScbLLJjiI+clbQGeAWrAUET0dyMpMytW2a+4ulFW3xYRC120zKaGCDEcv5RryUPSYkkbJW2SdHGL+EslfVXSA5LWSzq30zHdVDSzEbLO+e488iOpD7gaOAkYBFZLWhkRDzdsdgHwcET8rqSXARsl3RQRu9sdd6JXXAH8p6T7JC1tk/hSSWskrdnDCxM8nZn1XjbnfJ4lh2OBTRGxuV6IbgFOa9omgFmSBBwI7AKGUged6BXXCRGxVdIhwDcl/SAi7hyRUcQAMADwEs2OCZ7PzHos65zP3cc1R9KahvWB+n/zex0GPNawPggc13SMTwMrga3ALODdEZG8czOhwhURW+t/d0j6Cll1vTO9l5mV3RhGzu/s0L/dqgI2X8C8A1gL/DZwJNlF0F0R8XS7g467qSjpAEmz9n4GTgbWjfd4ZlYOe0fO51lyGATmNawfTnZl1ehc4NbIbAJ+DBydOuhErrgOBb6SNUuZBnwuIr4xgeNNWX2vWZCMx/7Tk/Gtbz0oGf/F8e3HHM1+aXo80l1vSI9nKtLXfz4rGf/4pxcn4/e+/nNtYz/e84vkvpdvPykZ/9W7pnavRxdflrEaWCBpPvA4cBZwdtM2jwJvB+6SdCjwamBz6qDjLlwRsRl4w3j3N7NyioA9w90pXBExJGkZcDvQB6yIiPWSzqvHlwP/AFwv6SGypuVFEbEzdVwPhzCzEbKmYvdGzkfEKmBV03fLGz5vJetqys2Fy8xGKfvIeRcuMxthjMMhCuHCZWZNuttU7AUXLjMbxXPO7wNqi96UjF9x/dXJ+FHT20+/MpXtiVoy/rdXvT8Zn/ZcekjCb35xWdvYrMeTT5Sw/870cImZa+5Nxqssu6vo15OZWYV46mYzqyQ3Fc2sUnxX0cwqyXcVzaxSIsSQC5eZVY2bimZWKe7j2kfsv7F5eqGR7nt+XjJ+1PTt3Uynqy7cdnwyvvnZ9OvNrj/yS21jTw2nx2Ed+qn/TsZ7aWpPWtOZC5eZVYrHcZlZJXkcl5lVSgQMdWkiwV5x4TKzUdxUNLNKcR+XmVVSuHCZWdW4c34fMLTtJ8n4VR8/Mxn/2OL0K8T6HjwwGX/gw1cl4ykf3fnryfim35mZjNee3JaMn/2bH24b2/InyV2ZzwPpDawnItzHZWaVI2q+q2hmVeM+LjOrFD+raGbVE1k/V5m5cJnZKL6raGaVEu6cN7MqclPRmP1vdyfjL/vqryTjtSd2JeOvfd0fto2tP3FFct+VA29Nxg95cmJzYunu9mOx5qd/FitQ2e8qdrwelLRC0g5J6xq+my3pm5J+WP97cG/TNLPJEpEVrjxLUfI0ZK8HFjd9dzFwR0QsAO6or5vZFDEcyrUUpWPhiog7gea2ymnADfXPNwCndzctMytSRL6lKOPt4zo0IrYBRMQ2SYe021DSUmApwAzSz72ZWfECMVzyu4o9zy4iBiKiPyL6p7N/r09nZl0QOZeijLdwbZc0F6D+d0f3UjKzQnW5c17SYkkbJW2S1LI/XNIiSWslrZf0nU7HHG/hWgmcU/98DnDbOI9jZmXUpUsuSX3A1cApwDHAEknHNG1zEPAZ4Pci4rVAeh4ocvRxSboZWATMkTQIXApcDnxB0geAR/OcyNqr7XxiQvvveXq/ce/72vc8nIz/9Jq+9AGGa+M+t5VXF4c6HAtsiojNAJJuIbu51/gv3tnArRHxaHbu6NiC61i4ImJJm9DbO+1rZtUTwPBw7sI1R9KahvWBiBhoWD8MeKxhfRA4rukYRwHTJX0bmAVcGRE3pk7qkfNmNlIA+a+4dkZEfyLe6kDNjcxpwJvJLoZ+Gbhb0j0R8Ui7g7pwmdkoXRyjNQjMa1g/HNjaYpudEfEc8JykO4E3AG0LV7kHa5hZMbo3HmI1sEDSfEn7AWeR3dxrdBvwW5KmSZpJ1pTckDqor7jMrEn3nkOMiCFJy4DbgT5gRUSsl3RePb48IjZI+gbwIDAMXBcR69of1YXLzFrp4ujSiFgFrGr6bnnT+j8D/5z3mC5cU8BrLmrbFcC5r0/f/P23V96RjL/1zAuS8VmfvycZtwoKiPx3FQvhwmVmLbhwmVnVeAZUM6scFy4zq5SxDUAthAuXmY3il2WYWfX4rqKZVY18xWW9VnvyqbaxJ85/TXLfR1f+Ihm/+KPJh/T563edkYzH/7y0bWzexzq8n6zs7ZWpqujpTXNw4TKzJnLnvJlVkK+4zKxyhotOIM2Fy8xG8jguM6si31U0s+opeeHyDKhmVjm+4prihh9IzoDLWX/3l8n4TZd+Ihlfe3x6nBfHtw+99oBlyV0XXLstGR/avCV9bhs3NxXNrFoCP/JjZhXkKy4zqxo3Fc2sely4zKxyXLjMrEoUbiqaWRX5rqKV2ewV6Tmxlm1Mv1fxJZcPJuM3/9rtbWPr3/fp5L5Hz/tgMv7qv0uPn679cHMybu2V/Yqr48h5SSsk7ZC0ruG7yyQ9LmltfTm1t2ma2aSKnEtB8jzycz2wuMX3/xIRC+vLqhZxM6uieLGfq9NSlI6FKyLuBHZNQi5mVhZT4IqrnWWSHqw3JQ9ut5GkpZLWSFqzhxcmcDozmywazrcUZbyF6xrgSGAhsA34ZLsNI2IgIvojon86+4/zdGZmLxpX4YqI7RFRi4hh4Frg2O6mZWaFmopNRUlzG1bPANa129bMKqYCnfMdx3FJuhlYBMyRNAhcCiyStJCs5m4BPtS7FK1I+t7aZPznf3BIMv4b7/7jtrF7L7oyue8P3nZdMv6eI05Oxp96SzJsKSUfx9WxcEXEkhZff7YHuZhZWVS9cJnZvkUUe8cwD885b2YjdbmPS9JiSRslbZJ0cWK735BUk/QHnY7pwmVmo3XprqKkPuBq4BTgGGCJpGPabPdxoP3DrQ1cuMxstO4NhzgW2BQRmyNiN3ALcFqL7f4Y+DKwI89BXbjMbJQxNBXn7H0ypr4sbTrUYcBjDeuD9e9ePJd0GNmwquV583PnvE1IbXv6f5CHfqp9/Pm/GkruO1P7JePXHvG1ZPydZ3yk/bG/cm9y331e/ruKOyOiPxFvNbFX89H/FbgoImpSvnnAXLjMbKTo6l3FQWBew/rhwNambfqBW+pFaw5wqqShiPj3dgd14TKz0bo3jms1sEDSfOBx4Czg7BGnipi/97Ok64GvpYoWuHCZWQvdepwnIoYkLSO7W9gHrIiI9ZLOq8dz92s1cuEys9G6OHK+PtHoqqbvWhasiHh/nmO6cJnZSAXP/JCHC5eZjSDK/7IMFy4zG8WFyypt+C0Lk/EfnTkjGX/dwi1tY53GaXVy1a43JuMzb1szoePv01y4zKxyXLjMrFIKnt00DxcuMxvNhcvMqqbsEwm6cJnZKG4qmlm1eACqmVWSC5cVSf2vS8Yf+ZMOc16dcEMyfuKM3WPOKa8XYk8yfs+u+ck4w9u6mM2+wyPnzaySNFzuyuXCZWYjuY/LzKrITUUzqx4XLjOrGl9xmVn1uHCZWaV09y0/PdGxcEmaB9wIvBwYBgYi4kpJs4HPA0cAW4B3RcTPepfqvmva/Fcm4z8691fbxi579y3JfX//wJ3jyqkbLtmeeh0ffOfK45Pxg2+4u5vpWF0VxnHleZP1EHBhRLwGOB64QNIxwMXAHRGxALijvm5mU0FEvqUgHQtXRGyLiPvrn58BNpC9Qvs0YO+w6huA03uUo5lNMkW+pShj6uOSdATwRuBe4NCI2AZZcZN0SPfTM7NJN5UGoEo6EPgy8JGIeLr+uuw8+y0FlgLMYOZ4cjSzSVb2zvk8fVxImk5WtG6KiFvrX2+XNLcenwvsaLVvRAxERH9E9E9n/27kbGY9puF8S1E6Fi5ll1afBTZExBUNoZXAOfXP5wC3dT89M5t0Qek75/M0FU8A3gs8JGlt/btLgMuBL0j6APAocGZPMpwCph3ximT8qTfPTcbf/fffSMbPO+jWZLyXLtyWHrJw92faD3mYff33k/sePOzhDkUp+3CIjoUrIr5LNrSjlbd3Nx0zK4WqFy4z27dUYQCqC5eZjRThiQTNrILKXbdcuMxsNDcVzaxaAnBT0cwqp9x1y4Urr2lzX942tmvFAcl9z5//nWR8yazt48qpG5Y9/pZk/P5rFibjc760Lhmf/YzHYlVRN5uKkhYDVwJ9wHURcXlT/D3ARfXVZ4HzI+KB1DFduMxslG7dVZTUB1wNnAQMAqslrYyIhxs2+zHw1oj4maRTgAHguNRxcz2raGb7kBjD0tmxwKaI2BwRu4FbyKbEevF0Ef/dMAnpPcDhnQ7qKy4zGyEbgJr7imuOpDUN6wMRMdCwfhjwWMP6IOmrqQ8AX+90UhcuMxst/8wPOyMiNQd3q8cFW1ZFSW8jK1zpjldcuMyshTFccXUyCMxrWD8c2DrqfNKvA9cBp0TEE50O6j4uMxupu31cq4EFkuZL2g84i2xKrP8n6RXArcB7I+KRPAf1FZeZNenes4oRMSRpGXA72XCIFRGxXtJ59fhy4G+BXwE+U59ZeahD83PfKVy735F+FdbuP9uVjF/yqlVtYyf/8nPjyqlbttd+0TZ24soLk/se/Tc/SMZnP5keh1XyGX5tvLo4SWBErAJWNX23vOHzB4EPjuWY+0zhMrOcpsILYc1sH1TgtMx5uHCZ2WjlrlsuXGY2mobL3VZ04TKzkYLS33Vx4TKzEUR0cwBqT7hwmdloLlzlsOX09EMCj7z+iz0799VPHpmMX/mdk5Nx1dq9HS5z9Ed/3Da2YPu9yX1ryajts1y4zKxS3MdlZlXku4pmVjHhpqKZVUzgwmVmFVTulqILl5mN5nFcZlY9VS9ckuYBNwIvJ7uAHIiIKyVdBvwR8NP6ppfU590ppaPO/34y/s7z3zxJmYx2FOncOvFYLOuqCKiVu62Y54prCLgwIu6XNAu4T9I367F/iYhP9C49MytE1a+4ImIbsK3++RlJG8heOWRmU1XJC9eYXpYh6QjgjcDe50iWSXpQ0gpJB7fZZ6mkNZLW7OGFiWVrZr0XwHDkWwqSu3BJOhD4MvCRiHgauAY4ElhIdkX2yVb7RcRARPRHRP909p94xmbWYwExnG8pSK67ipKmkxWtmyLiVoCI2N4Qvxb4Wk8yNLPJFZS+c77jFZey9wV9FtgQEVc0fD+3YbMzgHXdT8/MChGRbylIniuuE4D3Ag9JWlv/7hJgiaSFZPV5C/ChHuRnZkUoeed8nruK3wVaTQhV2jFbZjYRfsjazKomAE9rY2aV4ysuM6uWqfHIj5ntSwKiwDFaebhwmdloBY6Kz8OFy8xGcx+XmVVKhO8qmlkF+YrLzKoliFq5p6d04TKzkfZOa1NiLlxmNlrJh0OMaSJBM5v6AojhyLXkIWmxpI2SNkm6uEVckj5Vjz8o6U2djunCZWYjRfcmEpTUB1wNnAIcQzarzDFNm50CLKgvS8kmKU1y4TKzUaJWy7XkcCywKSI2R8Ru4BbgtKZtTgNujMw9wEFN8/2NMql9XM/ws53fii/9b8NXc4Cdk5nDGJQ1t7LmBc5tvLqZ2ysneoBn+Nnt34ovzcm5+QxJaxrWByJioGH9MOCxhvVB4LimY7Ta5jDqL+lpZVILV0S8rHFd0pqI6J/MHPIqa25lzQuc23iVLbeIWNzFw7Way6+5cyzPNiO4qWhmvTQIzGtYPxzYOo5tRnDhMrNeWg0skDRf0n7AWcDKpm1WAu+r3108Hniq/j7XtooexzXQeZPClDW3suYFzm28ypzbhETEkKRlwO1AH7AiItZLOq8eX042DfypwCbg58C5nY6rKPkzSWZmzdxUNLPKceEys8oppHB1egSgSJK2SHpI0tqm8SlF5LJC0g5J6xq+my3pm5J+WP97cIlyu0zS4/Xfbq2kUwvKbZ6k/5K0QdJ6SX9a/77Q3y6RVyl+tyqZ9D6u+iMAjwAnkd0GXQ0siYiHJzWRNiRtAfojovDBipJOBJ4lG1X8uvp3/wTsiojL60X/4Ii4qCS5XQY8GxGfmOx8mnKbC8yNiPslzQLuA04H3k+Bv10ir3dRgt+tSoq44srzCIABEXEnsKvp69OAG+qfbyD7F3/StcmtFCJiW0TcX//8DLCBbCR2ob9dIi8boyIKV7vh/WURwH9Kuk/S0qKTaeHQvWNc6n8PKTifZsvqT/ivKKoZ20jSEcAbgXsp0W/XlBeU7HcruyIK15iH90+yEyLiTWRPrF9QbxJZPtcARwILyZ4z+2SRyUg6EPgy8JGIeLrIXBq1yKtUv1sVFFG4xjy8fzJFxNb63x3AV8iatmWyfe+T8/W/OwrO5/9FxPaIqEX2Ur5rKfC3kzSdrDjcFBG31r8u/LdrlVeZfreqKKJw5XkEoBCSDqh3miLpAOBkYF16r0m3Ejin/vkc4LYCcxmhaSqSMyjot5Mk4LPAhoi4oiFU6G/XLq+y/G5VUsjI+frt3n/lxUcAPjbpSbQg6dfIrrIgexzqc0XmJulmYBHZtCfbgUuBfwe+ALwCeBQ4MyImvZO8TW6LyJo7AWwBPtTpmbMe5fYW4C7gIWDvbHeXkPUnFfbbJfJaQgl+tyrxIz9mVjkeOW9mlePCZWaV48JlZpXjwmVmlePCZWaV48JlZpXjwmVmlfN/3EbB+LwAwcEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "\n",
    "for i in [0, 101]:\n",
    "    im,label = train_dataset[i]\n",
    "    \n",
    "    plt.imshow(im.numpy()[0])\n",
    "    plt.title(label)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "print(im.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define network\n",
    "\n",
    "Next we define the VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 1**: comple the PyTorch VAE model below by implementing the `reparameterize` method which samples from the approximate posterior using the reparameterisation trick, and implementing the model's `forward` method, which returns the estimated parameters $f$,$g$ and $h$ of the likelihood and approximate posterior given a sample $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)# estimates mu, log-variance of approximate posterior q(z|x)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "        # TODO: write some code which returns a random sample from the approximate posterior q(z|x) here\n",
    "        \n",
    "\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))# estimates mu of likelihood p(x|z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "        # TODO: write some code which returns\n",
    "        # 1) mu,log-variance of the approximate posterior given x\n",
    "        # 2) mu of the likelihood given a random sample from the posterior given x\n",
    "        \n",
    "\n",
    "        \n",
    "        return recon, mu, logvar# should return  (likelihood mu, posterior mu, posterior log-variance)\n",
    "    \n",
    "model = VAE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "Next, we define the loss function to train the VAE model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 2**: define a loss function to train the VAE model, which minimises the expression for the negative ELBO shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon, x, mu, logvar):\n",
    "    \n",
    "    BCE = F.binary_cross_entropy(recon, x.view(-1, 784), reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Finally, we train the VAE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)# this automatically batches up examples, adding a batch dimension\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 3**: write some code to train the VAE using the training data and the loss function defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 109.120392\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 103.371941\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 102.188370\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 111.986038\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 109.921188\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 102.488220\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 107.942802\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 109.414200\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 106.431160\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 102.938583\n",
      "====> Epoch: 1 Average loss: 108.0693\n",
      "====> Test set loss: 106.8609\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 106.235138\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 107.749176\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 105.837807\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 105.285507\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 108.019684\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 105.687309\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 104.912621\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 100.939232\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 103.570343\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 108.091965\n",
      "====> Epoch: 2 Average loss: 107.2889\n",
      "====> Test set loss: 106.3931\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 104.059814\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 101.210091\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 106.956863\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 108.782257\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 109.019913\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 101.456947\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 107.285362\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 108.116470\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 103.728661\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 107.655495\n",
      "====> Epoch: 3 Average loss: 106.6939\n",
      "====> Test set loss: 106.2903\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 115.470757\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 104.437683\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 101.838699\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 111.625679\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 106.873085\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 105.615379\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 103.201828\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 103.708206\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 104.413818\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 106.513191\n",
      "====> Epoch: 4 Average loss: 106.1553\n",
      "====> Test set loss: 105.6008\n"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        \n",
    "        # TODO: write some code here\n",
    "        # carry out an optimisation step for each batch\n",
    "        # after each epoch report the average loss over the training dataset\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "for epoch in range(1, 5):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "In this section we evaluate the performance of the trained VAE.\n",
    "\n",
    "### Reconstruction quality\n",
    "\n",
    "> **Task 4**: plot example VAE reconstructions (mean of the likelihood) for some of the images in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.4384e-07, 1.5234e-07, 1.0379e-07,  ..., 7.7273e-08,\n",
      "           5.1871e-08, 1.1689e-07],\n",
      "          [2.2406e-07, 5.3008e-08, 9.0643e-08,  ..., 1.2301e-07,\n",
      "           1.2163e-07, 1.2209e-07],\n",
      "          [2.5969e-07, 9.7767e-08, 7.0479e-08,  ..., 1.5486e-06,\n",
      "           1.7523e-07, 4.9544e-08],\n",
      "          ...,\n",
      "          [7.2857e-08, 1.4418e-07, 6.8121e-07,  ..., 3.2214e-07,\n",
      "           1.4336e-06, 9.1033e-08],\n",
      "          [7.8043e-08, 3.5141e-08, 1.2250e-07,  ..., 1.2828e-06,\n",
      "           1.5655e-07, 4.9021e-08],\n",
      "          [5.4788e-08, 8.2004e-08, 1.1252e-07,  ..., 3.8187e-08,\n",
      "           8.2464e-08, 7.0480e-08]]],\n",
      "\n",
      "\n",
      "        [[[2.2193e-07, 1.7677e-07, 1.1069e-07,  ..., 2.5456e-07,\n",
      "           5.2054e-08, 1.3452e-07],\n",
      "          [1.8998e-07, 1.3311e-07, 8.0170e-08,  ..., 7.4151e-08,\n",
      "           1.1974e-07, 1.0953e-07],\n",
      "          [8.9656e-08, 5.1628e-08, 3.5492e-08,  ..., 6.1479e-09,\n",
      "           1.1153e-07, 1.0476e-07],\n",
      "          ...,\n",
      "          [1.2208e-07, 7.6395e-08, 1.0688e-06,  ..., 3.1270e-07,\n",
      "           9.7192e-07, 8.9867e-08],\n",
      "          [6.3065e-08, 1.5014e-07, 1.6843e-07,  ..., 3.2178e-06,\n",
      "           1.3775e-07, 1.0972e-07],\n",
      "          [1.2656e-07, 1.0792e-07, 7.8022e-08,  ..., 1.2938e-07,\n",
      "           7.0404e-08, 6.3380e-08]]],\n",
      "\n",
      "\n",
      "        [[[1.6802e-09, 8.9851e-10, 1.0496e-09,  ..., 1.0994e-09,\n",
      "           8.9177e-10, 2.1515e-09],\n",
      "          [2.5350e-09, 4.6768e-10, 7.1560e-10,  ..., 2.9910e-09,\n",
      "           2.0876e-09, 1.6819e-09],\n",
      "          [5.2231e-10, 1.0722e-09, 9.3103e-09,  ..., 1.7532e-09,\n",
      "           9.5605e-10, 6.6616e-10],\n",
      "          ...,\n",
      "          [9.5446e-10, 7.8409e-10, 2.3094e-08,  ..., 1.0585e-08,\n",
      "           1.2358e-08, 1.5656e-09],\n",
      "          [1.4368e-09, 5.0157e-10, 5.8700e-10,  ..., 3.0375e-09,\n",
      "           3.3463e-09, 1.0245e-09],\n",
      "          [9.3925e-10, 1.5046e-09, 2.5180e-09,  ..., 8.7015e-10,\n",
      "           7.7099e-10, 1.5350e-09]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[6.9962e-07, 3.1937e-07, 2.5612e-07,  ..., 3.2037e-07,\n",
      "           1.7653e-07, 6.1929e-07],\n",
      "          [1.9430e-07, 2.0012e-07, 1.2887e-07,  ..., 4.3343e-07,\n",
      "           4.2735e-07, 7.9839e-07],\n",
      "          [4.5655e-07, 1.4782e-07, 4.0718e-07,  ..., 1.1108e-06,\n",
      "           3.2302e-07, 1.7103e-07],\n",
      "          ...,\n",
      "          [4.0386e-07, 2.3876e-07, 4.6348e-07,  ..., 1.5887e-06,\n",
      "           2.8587e-06, 1.4605e-07],\n",
      "          [2.8278e-07, 1.8376e-07, 2.6865e-07,  ..., 2.7462e-07,\n",
      "           2.2386e-07, 2.0774e-07],\n",
      "          [2.0986e-07, 3.9294e-07, 2.0169e-07,  ..., 4.5620e-07,\n",
      "           2.7228e-07, 3.5371e-07]]],\n",
      "\n",
      "\n",
      "        [[[3.6101e-09, 1.1247e-09, 3.4156e-09,  ..., 9.8705e-10,\n",
      "           5.5304e-09, 1.0118e-08],\n",
      "          [3.1620e-09, 2.3204e-09, 3.6682e-09,  ..., 6.2972e-09,\n",
      "           4.7405e-09, 5.8640e-09],\n",
      "          [3.3310e-09, 2.1787e-09, 3.6924e-09,  ..., 2.3129e-08,\n",
      "           4.1040e-09, 1.8819e-09],\n",
      "          ...,\n",
      "          [4.8420e-09, 3.6092e-09, 1.0162e-08,  ..., 3.8120e-09,\n",
      "           7.3214e-09, 1.7413e-09],\n",
      "          [2.6916e-09, 1.2375e-09, 2.9294e-09,  ..., 1.7287e-09,\n",
      "           1.8507e-09, 1.9060e-09],\n",
      "          [4.5828e-09, 2.4853e-09, 2.4228e-09,  ..., 2.7729e-09,\n",
      "           3.7314e-09, 2.9945e-09]]],\n",
      "\n",
      "\n",
      "        [[[2.0858e-07, 2.1368e-07, 1.4084e-07,  ..., 2.1828e-07,\n",
      "           2.5512e-07, 1.8179e-07],\n",
      "          [2.2830e-07, 1.5069e-07, 2.0572e-07,  ..., 5.1307e-07,\n",
      "           1.8988e-07, 2.6904e-07],\n",
      "          [2.0023e-07, 2.7060e-07, 1.4380e-07,  ..., 6.2458e-07,\n",
      "           3.1915e-07, 1.5871e-07],\n",
      "          ...,\n",
      "          [1.6326e-07, 1.2455e-07, 1.3636e-06,  ..., 1.1489e-06,\n",
      "           1.9420e-06, 1.6846e-07],\n",
      "          [1.0899e-07, 1.8050e-07, 9.4776e-08,  ..., 6.6165e-07,\n",
      "           2.4001e-07, 1.0278e-07],\n",
      "          [2.2312e-07, 2.1623e-07, 2.2311e-07,  ..., 1.1099e-07,\n",
      "           2.5675e-07, 1.5513e-07]]]], grad_fn=<ViewBackward>)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (64, 1, 28, 28) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-5d45c922c980>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aml-practical/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2722\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m         data=None, **kwargs):\n\u001b[0;32m-> 2724\u001b[0;31m     __ret = gca().imshow(\n\u001b[0m\u001b[1;32m   2725\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2726\u001b[0m         \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aml-practical/lib/python3.8/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1447\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aml-practical/lib/python3.8/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5521\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5523\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5524\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5525\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aml-practical/lib/python3.8/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    709\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    710\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 711\u001b[0;31m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0m\u001b[1;32m    712\u001b[0m                             .format(self._A.shape))\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (64, 1, 28, 28) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: write some code here\n",
    "sample = torch.randn(64, 20)\n",
    "sample = model.decode(sample).view(64,1,28,28)\n",
    "print(sample)\n",
    "plt.imshow(sample.detach().numpy()[0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent space interpolation\n",
    "\n",
    "Now that we have learnt a latent representation, we can interpolate smoothly between two digits by linearly interpolating between them in the latent space.\n",
    "\n",
    "> **Task 5**: show the mean of the likelihood (image reconstruction) for a set latent vectors which are linearly interpolated between two test images in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: write some code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random sampling of the posterior\n",
    "\n",
    "We can also generate random samples of $x$ given values of $z$ drawn from its prior distribution (a unit Gaussian).\n",
    "\n",
    "> **Task 6**: generate random latent vectors from the prior latent distribution and show the resulting likelihood means (image reconstructions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: write some code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "Have a go at these if you have time.\n",
    "\n",
    "> What happens to the performance when you reduce the dimensionality of the latent space?\n",
    "\n",
    "> Try using a Beta-VAE (https://openreview.net/forum?id=Sy2fzU9gl) : what happens when the KL term is made much stronger?\n",
    "\n",
    "> Try using a binary cross-entropy loss instead of a L2 loss in the loss function (e.g. https://github.com/pytorch/examples/blob/master/vae/main.py)\n",
    "\n",
    "> Try to run clustering in the latent space to label the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
