{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "covered-visiting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.utils.convert import from_networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "respective-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1\n",
    "\n",
    "#Task 1.1\n",
    "def gen_cycle_pairs(ns):\n",
    "    graphs = []\n",
    "    \n",
    "    #generate pairs between 3 and n-3 such that their sum equals n\n",
    "    def gen_pairs(n):\n",
    "        res = []\n",
    "        for i in range(3, n-2):\n",
    "            res.append((i, n-i))\n",
    "        return res\n",
    "    \n",
    "    #iterate over all ns \n",
    "    for n in ns:\n",
    "        \n",
    "        #generate pairs for given n and iterate over all pairs\n",
    "        cycles = gen_pairs(n)\n",
    "        for cycle in cycles:\n",
    "            #for each pair generate the corresponing graphs\n",
    "            c1 = nx.cycle_graph(cycle[0])\n",
    "            c2 = nx.cycle_graph(cycle[1])\n",
    "            graphs.append(nx.disjoint_union(c1, c2))\n",
    "            \n",
    "            #also append a single cycle graph for balance\n",
    "            graphs.append(nx.cycle_graph(n))  \n",
    "    \n",
    "    return graphs\n",
    "    \n",
    "nx_graphs = gen_cycle_pairs(range(6, 16))\n",
    "\n",
    "#Sanity check\n",
    "for i in range(0, 110,2):  \n",
    "    assert(nx.algorithms.graph_hashing.weisfeiler_lehman_graph_hash(nx_graphs[i]) \n",
    "        == nx.algorithms.graph_hashing.weisfeiler_lehman_graph_hash(nx_graphs[i+1]))\n",
    "\n",
    "    \n",
    "#Task 1.2\n",
    "py_graphs = []\n",
    "for i, graph in enumerate(nx_graphs):\n",
    "\n",
    "    py_graph = from_networkx(graph)\n",
    "    \n",
    "    #x features\n",
    "    py_graph.x = torch.zeros(py_graph.num_nodes,50)\n",
    "    \n",
    "    #for all even indices the graph is not a simple cycle\n",
    "    if (i%2 == 0):\n",
    "        py_graph.y = torch.tensor([0])\n",
    "        \n",
    "    #for all odd indices the graph is a simple cycle\n",
    "    else:\n",
    "        py_graph.y = torch.tensor([1])\n",
    "        \n",
    "    py_graphs.append(py_graph)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-enzyme",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonaswildberger/Library/Python/3.8/lib/python/site-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.69380\n",
      "Epoch 002: | Loss: 0.69354\n",
      "Epoch 003: | Loss: 0.69482\n",
      "Epoch 004: | Loss: 0.69631\n",
      "Epoch 005: | Loss: 0.69285\n",
      "Epoch 006: | Loss: 0.69359\n",
      "Epoch 007: | Loss: 0.69486\n",
      "Epoch 008: | Loss: 0.69558\n",
      "Epoch 009: | Loss: 0.69373\n",
      "Epoch 010: | Loss: 0.69332\n",
      "Epoch 011: | Loss: 0.69449\n",
      "Epoch 012: | Loss: 0.69541\n",
      "Epoch 013: | Loss: 0.69394\n",
      "Epoch 014: | Loss: 0.69426\n",
      "Epoch 015: | Loss: 0.69394\n",
      "Epoch 016: | Loss: 0.69344\n",
      "Epoch 017: | Loss: 0.69275\n",
      "Epoch 018: | Loss: 0.69223\n",
      "Epoch 019: | Loss: 0.69603\n",
      "Epoch 020: | Loss: 0.69357\n",
      "Epoch 021: | Loss: 0.69587\n",
      "Epoch 022: | Loss: 0.69303\n",
      "Epoch 023: | Loss: 0.69240\n",
      "Epoch 024: | Loss: 0.69588\n",
      "Epoch 025: | Loss: 0.69094\n",
      "Epoch 026: | Loss: 0.69583\n",
      "Epoch 027: | Loss: 0.69511\n",
      "Epoch 028: | Loss: 0.69700\n",
      "Epoch 029: | Loss: 0.69570\n",
      "Epoch 030: | Loss: 0.69378\n",
      "Epoch 031: | Loss: 0.69419\n",
      "Epoch 032: | Loss: 0.69232\n",
      "Epoch 033: | Loss: 0.69555\n",
      "Epoch 034: | Loss: 0.69371\n",
      "Epoch 035: | Loss: 0.69185\n",
      "Epoch 036: | Loss: 0.69083\n",
      "Epoch 037: | Loss: 0.69470\n",
      "Epoch 038: | Loss: 0.69476\n",
      "Epoch 039: | Loss: 0.69323\n",
      "Epoch 040: | Loss: 0.69373\n",
      "Epoch 041: | Loss: 0.69479\n",
      "Epoch 042: | Loss: 0.69434\n",
      "Epoch 043: | Loss: 0.69338\n",
      "Epoch 044: | Loss: 0.69407\n",
      "Epoch 045: | Loss: 0.69270\n",
      "Epoch 046: | Loss: 0.69569\n",
      "Epoch 047: | Loss: 0.69322\n",
      "Epoch 048: | Loss: 0.69464\n",
      "Epoch 049: | Loss: 0.69224\n",
      "Epoch 050: | Loss: 0.69182\n",
      "Epoch 051: | Loss: 0.69535\n",
      "Epoch 052: | Loss: 0.69574\n",
      "Epoch 053: | Loss: 0.69273\n",
      "Epoch 054: | Loss: 0.69287\n",
      "Epoch 055: | Loss: 0.69469\n",
      "Epoch 056: | Loss: 0.69415\n",
      "Epoch 057: | Loss: 0.69183\n",
      "Epoch 058: | Loss: 0.69467\n",
      "Epoch 059: | Loss: 0.69561\n",
      "Epoch 060: | Loss: 0.69467\n",
      "Epoch 061: | Loss: 0.69117\n",
      "Epoch 062: | Loss: 0.69490\n",
      "Epoch 063: | Loss: 0.69335\n",
      "Epoch 064: | Loss: 0.69156\n",
      "Epoch 065: | Loss: 0.69476\n",
      "Epoch 066: | Loss: 0.69471\n",
      "Epoch 067: | Loss: 0.69367\n",
      "Epoch 068: | Loss: 0.69364\n",
      "Epoch 069: | Loss: 0.69680\n",
      "Epoch 070: | Loss: 0.69311\n",
      "Epoch 071: | Loss: 0.69285\n",
      "Epoch 072: | Loss: 0.69279\n",
      "Epoch 073: | Loss: 0.69363\n",
      "Epoch 074: | Loss: 0.69232\n",
      "Epoch 075: | Loss: 0.69563\n",
      "Epoch 076: | Loss: 0.69448\n",
      "Epoch 077: | Loss: 0.69222\n",
      "Epoch 078: | Loss: 0.69250\n",
      "Epoch 079: | Loss: 0.69172\n",
      "Epoch 080: | Loss: 0.69331\n",
      "Epoch 081: | Loss: 0.69271\n",
      "Epoch 082: | Loss: 0.69348\n",
      "Epoch 083: | Loss: 0.69284\n",
      "Epoch 084: | Loss: 0.69345\n",
      "Epoch 085: | Loss: 0.69255\n",
      "Epoch 086: | Loss: 0.69229\n",
      "Epoch 087: | Loss: 0.69393\n",
      "Epoch 088: | Loss: 0.69469\n",
      "Epoch 089: | Loss: 0.69063\n",
      "Epoch 090: | Loss: 0.69383\n",
      "Epoch 091: | Loss: 0.69528\n",
      "Epoch 092: | Loss: 0.69437\n",
      "Epoch 093: | Loss: 0.69455\n",
      "Epoch 094: | Loss: 0.69424\n",
      "Epoch 095: | Loss: 0.69579\n",
      "Epoch 096: | Loss: 0.69503\n",
      "Epoch 097: | Loss: 0.69511\n",
      "Epoch 098: | Loss: 0.69481\n",
      "Epoch 099: | Loss: 0.69169\n",
      "Epoch 100: | Loss: 0.69218\n",
      "Epoch 101: | Loss: 0.69508\n",
      "Epoch 102: | Loss: 0.69146\n",
      "Epoch 103: | Loss: 0.69641\n",
      "Epoch 104: | Loss: 0.69183\n",
      "Epoch 105: | Loss: 0.69332\n",
      "Epoch 106: | Loss: 0.69293\n",
      "Epoch 107: | Loss: 0.69388\n",
      "Epoch 108: | Loss: 0.69372\n",
      "Epoch 109: | Loss: 0.68973\n",
      "Epoch 110: | Loss: 0.69283\n",
      "Epoch 111: | Loss: 0.69389\n",
      "Epoch 112: | Loss: 0.69385\n",
      "Epoch 113: | Loss: 0.69164\n",
      "Epoch 114: | Loss: 0.69306\n",
      "Epoch 115: | Loss: 0.69441\n",
      "Epoch 116: | Loss: 0.69197\n",
      "Epoch 117: | Loss: 0.69353\n",
      "Epoch 118: | Loss: 0.69710\n",
      "Epoch 119: | Loss: 0.69122\n",
      "Epoch 120: | Loss: 0.69219\n",
      "Epoch 121: | Loss: 0.69028\n",
      "Epoch 122: | Loss: 0.69444\n",
      "Epoch 123: | Loss: 0.69553\n",
      "Epoch 124: | Loss: 0.69397\n",
      "Epoch 125: | Loss: 0.69388\n",
      "Epoch 126: | Loss: 0.69412\n",
      "Epoch 127: | Loss: 0.69497\n",
      "Epoch 128: | Loss: 0.69535\n",
      "Epoch 129: | Loss: 0.69267\n",
      "Epoch 130: | Loss: 0.69286\n",
      "Epoch 131: | Loss: 0.69227\n",
      "Epoch 132: | Loss: 0.69488\n",
      "Epoch 133: | Loss: 0.69301\n",
      "Epoch 134: | Loss: 0.69356\n",
      "Epoch 135: | Loss: 0.69324\n",
      "Epoch 136: | Loss: 0.69609\n",
      "Epoch 137: | Loss: 0.69267\n",
      "Epoch 138: | Loss: 0.69375\n",
      "Epoch 139: | Loss: 0.69202\n",
      "Epoch 140: | Loss: 0.69490\n",
      "Epoch 141: | Loss: 0.69021\n",
      "Epoch 142: | Loss: 0.69487\n",
      "Epoch 143: | Loss: 0.69334\n",
      "Epoch 144: | Loss: 0.69526\n",
      "Epoch 145: | Loss: 0.69289\n",
      "Epoch 146: | Loss: 0.69256\n",
      "Epoch 147: | Loss: 0.69480\n",
      "Epoch 148: | Loss: 0.68948\n",
      "Epoch 149: | Loss: 0.69363\n",
      "Epoch 150: | Loss: 0.69296\n",
      "Epoch 151: | Loss: 0.69404\n",
      "Epoch 152: | Loss: 0.69154\n",
      "Epoch 153: | Loss: 0.69616\n",
      "Epoch 154: | Loss: 0.69351\n",
      "Epoch 155: | Loss: 0.69496\n",
      "Epoch 156: | Loss: 0.69288\n",
      "Epoch 157: | Loss: 0.69319\n",
      "Epoch 158: | Loss: 0.69448\n",
      "Epoch 159: | Loss: 0.69527\n",
      "Epoch 160: | Loss: 0.69170\n",
      "Epoch 161: | Loss: 0.69200\n",
      "Epoch 162: | Loss: 0.69388\n",
      "Epoch 163: | Loss: 0.69176\n",
      "Epoch 164: | Loss: 0.69080\n",
      "Epoch 165: | Loss: 0.69280\n",
      "Epoch 166: | Loss: 0.69393\n",
      "Epoch 167: | Loss: 0.69178\n",
      "Epoch 168: | Loss: 0.69654\n",
      "Epoch 169: | Loss: 0.69202\n",
      "Epoch 170: | Loss: 0.69712\n",
      "Epoch 171: | Loss: 0.69710\n",
      "Epoch 172: | Loss: 0.69519\n",
      "Epoch 173: | Loss: 0.69266\n",
      "Epoch 174: | Loss: 0.69449\n",
      "Epoch 175: | Loss: 0.69492\n",
      "Epoch 176: | Loss: 0.69125\n",
      "Epoch 177: | Loss: 0.69432\n",
      "Epoch 178: | Loss: 0.69290\n",
      "Epoch 179: | Loss: 0.69552\n",
      "Epoch 180: | Loss: 0.69115\n",
      "Epoch 181: | Loss: 0.69416\n",
      "Epoch 182: | Loss: 0.69088\n",
      "Epoch 183: | Loss: 0.69579\n",
      "Epoch 184: | Loss: 0.69216\n",
      "Epoch 185: | Loss: 0.69383\n",
      "Epoch 186: | Loss: 0.69604\n",
      "Epoch 187: | Loss: 0.69630\n",
      "Epoch 188: | Loss: 0.69427\n",
      "Epoch 189: | Loss: 0.69410\n",
      "Epoch 190: | Loss: 0.69105\n",
      "Epoch 191: | Loss: 0.69250\n",
      "Epoch 192: | Loss: 0.69233\n",
      "Epoch 193: | Loss: 0.69387\n",
      "Epoch 194: | Loss: 0.69185\n",
      "Epoch 195: | Loss: 0.69559\n",
      "Epoch 196: | Loss: 0.69458\n",
      "Epoch 197: | Loss: 0.69267\n",
      "Epoch 198: | Loss: 0.69171\n",
      "Epoch 199: | Loss: 0.68984\n",
      "Epoch 200: | Loss: 0.69109\n",
      "\n",
      "Fold 0: | Accuracy: 0.50000\n",
      "\n",
      "Epoch 001: | Loss: 0.69388\n",
      "Epoch 002: | Loss: 0.69450\n",
      "Epoch 003: | Loss: 0.69540\n",
      "Epoch 004: | Loss: 0.69261\n",
      "Epoch 005: | Loss: 0.69317\n",
      "Epoch 006: | Loss: 0.69517\n",
      "Epoch 007: | Loss: 0.69455\n",
      "Epoch 008: | Loss: 0.69210\n",
      "Epoch 009: | Loss: 0.69730\n",
      "Epoch 010: | Loss: 0.69505\n",
      "Epoch 011: | Loss: 0.69330\n",
      "Epoch 012: | Loss: 0.69309\n",
      "Epoch 013: | Loss: 0.69327\n",
      "Epoch 014: | Loss: 0.69373\n",
      "Epoch 015: | Loss: 0.69354\n",
      "Epoch 016: | Loss: 0.69734\n",
      "Epoch 017: | Loss: 0.69388\n",
      "Epoch 018: | Loss: 0.69209\n",
      "Epoch 019: | Loss: 0.69352\n",
      "Epoch 020: | Loss: 0.69406\n",
      "Epoch 021: | Loss: 0.69273\n",
      "Epoch 022: | Loss: 0.69279\n",
      "Epoch 023: | Loss: 0.69129\n",
      "Epoch 024: | Loss: 0.69267\n",
      "Epoch 025: | Loss: 0.69316\n",
      "Epoch 026: | Loss: 0.69230\n",
      "Epoch 027: | Loss: 0.69275\n",
      "Epoch 028: | Loss: 0.69590\n",
      "Epoch 029: | Loss: 0.69111\n",
      "Epoch 030: | Loss: 0.69419\n",
      "Epoch 031: | Loss: 0.69198\n",
      "Epoch 032: | Loss: 0.69217\n",
      "Epoch 033: | Loss: 0.69320\n",
      "Epoch 034: | Loss: 0.69049\n",
      "Epoch 035: | Loss: 0.69299\n",
      "Epoch 036: | Loss: 0.69424\n",
      "Epoch 037: | Loss: 0.69240\n",
      "Epoch 038: | Loss: 0.69420\n",
      "Epoch 039: | Loss: 0.69365\n",
      "Epoch 040: | Loss: 0.69364\n",
      "Epoch 041: | Loss: 0.69339\n",
      "Epoch 042: | Loss: 0.69316\n",
      "Epoch 043: | Loss: 0.69424\n",
      "Epoch 044: | Loss: 0.69443\n",
      "Epoch 045: | Loss: 0.69451\n",
      "Epoch 046: | Loss: 0.69160\n",
      "Epoch 047: | Loss: 0.69256\n",
      "Epoch 048: | Loss: 0.69451\n",
      "Epoch 049: | Loss: 0.69489\n",
      "Epoch 050: | Loss: 0.69343\n",
      "Epoch 051: | Loss: 0.69571\n",
      "Epoch 052: | Loss: 0.69145\n",
      "Epoch 053: | Loss: 0.69410\n",
      "Epoch 054: | Loss: 0.69346\n",
      "Epoch 055: | Loss: 0.69472\n",
      "Epoch 056: | Loss: 0.69487\n",
      "Epoch 057: | Loss: 0.69432\n",
      "Epoch 058: | Loss: 0.69385\n",
      "Epoch 059: | Loss: 0.69270\n",
      "Epoch 060: | Loss: 0.69482\n",
      "Epoch 061: | Loss: 0.69275\n",
      "Epoch 062: | Loss: 0.69193\n",
      "Epoch 063: | Loss: 0.69466\n",
      "Epoch 064: | Loss: 0.69370\n",
      "Epoch 065: | Loss: 0.69257\n",
      "Epoch 066: | Loss: 0.69393\n",
      "Epoch 067: | Loss: 0.69475\n",
      "Epoch 068: | Loss: 0.69555\n",
      "Epoch 069: | Loss: 0.69382\n",
      "Epoch 070: | Loss: 0.69534\n",
      "Epoch 071: | Loss: 0.69167\n",
      "Epoch 072: | Loss: 0.69486\n",
      "Epoch 073: | Loss: 0.69400\n",
      "Epoch 074: | Loss: 0.69577\n",
      "Epoch 075: | Loss: 0.69411\n",
      "Epoch 076: | Loss: 0.69477\n",
      "Epoch 077: | Loss: 0.69368\n",
      "Epoch 078: | Loss: 0.69414\n",
      "Epoch 079: | Loss: 0.69361\n",
      "Epoch 080: | Loss: 0.69381\n",
      "Epoch 081: | Loss: 0.69567\n",
      "Epoch 082: | Loss: 0.69266\n",
      "Epoch 083: | Loss: 0.69235\n",
      "Epoch 084: | Loss: 0.69555\n",
      "Epoch 085: | Loss: 0.69520\n",
      "Epoch 086: | Loss: 0.69515\n",
      "Epoch 087: | Loss: 0.69203\n",
      "Epoch 088: | Loss: 0.69287\n",
      "Epoch 089: | Loss: 0.69451\n",
      "Epoch 090: | Loss: 0.69247\n",
      "Epoch 091: | Loss: 0.69466\n",
      "Epoch 092: | Loss: 0.69394\n",
      "Epoch 093: | Loss: 0.69338\n",
      "Epoch 094: | Loss: 0.69364\n",
      "Epoch 095: | Loss: 0.69388\n",
      "Epoch 096: | Loss: 0.69210\n",
      "Epoch 097: | Loss: 0.69415\n",
      "Epoch 098: | Loss: 0.69363\n",
      "Epoch 099: | Loss: 0.69257\n",
      "Epoch 100: | Loss: 0.69179\n",
      "Epoch 101: | Loss: 0.69232\n",
      "Epoch 102: | Loss: 0.69175\n",
      "Epoch 103: | Loss: 0.69282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104: | Loss: 0.69450\n",
      "Epoch 105: | Loss: 0.69463\n",
      "Epoch 106: | Loss: 0.69530\n",
      "Epoch 107: | Loss: 0.69292\n",
      "Epoch 108: | Loss: 0.69320\n",
      "Epoch 109: | Loss: 0.69348\n"
     ]
    }
   ],
   "source": [
    "#Part 2\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as tg_nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "#Task 2.1\n",
    "\n",
    "input_dim = 50\n",
    "output_dim = 1\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #ModuleList of 16 Message-passing layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(tg_nn.GCNConv(input_dim, 50))\n",
    "        \n",
    "        self.num_MP_layers = 16\n",
    "        \n",
    "        for i in range(15):\n",
    "            self.convs.append(tg_nn.GCNConv(50, 50))\n",
    "        \n",
    "        #MLPs for post processing\n",
    "        self.MLPs = nn.Sequential(\n",
    "            nn.Linear(50, 50), nn.Dropout(0.25),\n",
    "            nn.Linear(50,50), nn.Dropout(0.1),\n",
    "            nn.Linear(50, output_dim)\n",
    "            )\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        #forwards throug MPNNs\n",
    "        for i in range(self.num_MP_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, 0.25)\n",
    "        \n",
    "        #global mean pool\n",
    "        x = tg_nn.global_mean_pool(x, batch)\n",
    "        \n",
    "        #MLPs\n",
    "        x = self.MLPs(x)     \n",
    "        return x\n",
    "    \n",
    "    #reset parameters of every parameterised layer\n",
    "    def reset_parameters(self):\n",
    "        for layers in self.children():\n",
    "            for layer in layers:\n",
    "                if hasattr(layer, 'reset_parameters'):\n",
    "                    layer.reset_parameters()\n",
    "    \n",
    "    #one output feature -> BCE loss\n",
    "    def loss(self, pred, label):       \n",
    "        return F.binary_cross_entropy(F.sigmoid(pred), label.float().unsqueeze(1))\n",
    "    \n",
    "#Task 2.2\n",
    "\n",
    "def train(net, training_loader, epochs):\n",
    "    \n",
    "    #Adam with 1.e-4 learning rate\n",
    "    opt = torch.optim.Adam(net.parameters(), lr = 1.e-4)\n",
    "\n",
    "    for epoch in range(1,epochs+1):\n",
    "        total_loss = 0\n",
    "        net.train()\n",
    "        for batch in training_loader:\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            prediction = net(batch)          \n",
    "            label = batch.y\n",
    "            \n",
    "            loss = net.loss(prediction, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "            \n",
    "        total_loss /= len(training_loader.dataset)\n",
    "        \n",
    "        print(f'Epoch {epoch+0:03}: | Loss: {total_loss:.5f}')\n",
    "\n",
    "def test(net,loader):\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    correct = 0\n",
    "\n",
    "    for data in loader:\n",
    "        with torch.no_grad():\n",
    "            output = net(data)\n",
    "            \n",
    "            #sigmoid(output) > 0.5 -> true, else false\n",
    "            classification = torch.round(F.sigmoid(output))\n",
    "            label = data.y\n",
    "            #print('classification', classification, 'label', label)\n",
    "        \n",
    "        correct+= (classification == label).sum()\n",
    "\n",
    "    return correct/ len(loader.dataset)\n",
    "\n",
    "def cross_validation(net, dataset, epochs):\n",
    "    data_size = len(dataset)\n",
    "    fold_index = int(0.2*len(dataset))\n",
    "    \n",
    "    mean_accuracy = 0\n",
    "    for k in range(5):\n",
    "        \n",
    "        #reset parameters after starting new fold\n",
    "        net.reset_parameters()\n",
    "        \n",
    "        #use 6 indices for left and right bound of the three data chunks\n",
    "        #[train_index_left_left, train_index_left_right],[test_index_left, test_index_right],\n",
    "        #[train_index_right_left, train_index_right_right]\n",
    "        \n",
    "        train_index_left_left = 0\n",
    "        train_index_left_right = k * fold_index\n",
    "\n",
    "        test_index_left = train_index_left_right\n",
    "        test_index_right = test_index_left + fold_index\n",
    "\n",
    "        train_index_right_left = test_index_right\n",
    "        train_index_right_right = data_size\n",
    "\n",
    "        #training set is union of two training chunks\n",
    "        train_indices = list(range(train_index_left_left, train_index_left_right))+ list(range(train_index_right_left, train_index_right_right))\n",
    "        test_indices = list(range(test_index_left, test_index_right))\n",
    "\n",
    "        train_set = torch.utils.data.dataset.Subset(dataset,train_indices)\n",
    "        test_set = torch.utils.data.dataset.Subset(dataset, test_indices)\n",
    "\n",
    "        train_loader = DataLoader(train_set, shuffle = True)\n",
    "        test_loader = DataLoader(test_set, shuffle = True)\n",
    "\n",
    "        train(net, train_loader, epochs)\n",
    "        \n",
    "        accuracy = test(net, test_loader)\n",
    "        mean_accuracy+= accuracy\n",
    "        print(f'\\nFold {k}: | Accuracy: {accuracy:.5f}\\n')\n",
    "        \n",
    "    print('Total Mean Accuracy', mean_accuracy/5)\n",
    "\n",
    "\n",
    "net = Network()\n",
    "net = cross_validation(net, py_graphs, 200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-liberty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3\n",
    "\n",
    "#Tast 3.1\n",
    "class Network_RNI(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(tg_nn.GCNConv(input_dim, 50)\n",
    "        )\n",
    "        \n",
    "        self.num_MP_layers = 16\n",
    "        \n",
    "        for i in range(15):\n",
    "            self.convs.append(tg_nn.GCNConv(50, 50))\n",
    "        \n",
    "        self.MLPs = nn.Sequential(\n",
    "            nn.Linear(50, 50), nn.Dropout(0.25),\n",
    "            nn.Linear(50,50), nn.Dropout(0.1),\n",
    "            nn.Linear(50,50), nn.Dropout(0.1),\n",
    "            nn.Linear(50, output_dim)\n",
    "            )\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        #sample 25 random features for ever data point\n",
    "        rvec = torch.randn((len(batch),25))\n",
    "        \n",
    "        x_pre = x.narrow(1, 0,25)\n",
    "        \n",
    "        #concatenate 25 zeros with 25 N(0,1) values for consecutive processing\n",
    "        x = torch.cat((x_pre, rvec), dim = 1)\n",
    "        #print(x)\n",
    "        \n",
    "        for i in range(0,self.num_MP_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, 0.25)\n",
    "            \n",
    "        x = tg_nn.global_max_pool(x, batch)\n",
    "        \n",
    "        x = self.MLPs(x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        for layers in self.children():\n",
    "            for layer in layers:\n",
    "                if hasattr(layer, 'reset_parameters'):\n",
    "                    layer.reset_parameters()\n",
    "    \n",
    "    def loss(self, pred, label):\n",
    "        return F.binary_cross_entropy(F.sigmoid(pred), label.float().unsqueeze(1))\n",
    "\n",
    "net_rni = Network_RNI()\n",
    "print(net_rni)\n",
    "\n",
    "cross_validation(net_rni, py_graphs, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network_RNI_adapted(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(tg_nn.GCNConv(input_dim, 50)\n",
    "        )\n",
    "        #4 MP layers\n",
    "        self.num_MP_layers = 4\n",
    "        \n",
    "        for i in range(3):\n",
    "            self.convs.append(tg_nn.GCNConv(50, 50))\n",
    "        \n",
    "        #8 MLP layers\n",
    "        self.MLPs = nn.Sequential(\n",
    "            nn.Linear(50, 50), nn.Dropout(0.25),\n",
    "            nn.Linear(50,50), nn.Dropout(0.1),\n",
    "            nn.Linear(50,50), nn.Dropout(0.1),\n",
    "            nn.Linear(50,50), nn.Dropout(0.1),\n",
    "            nn.Linear(50,50), nn.Dropout(0.1),\n",
    "            nn.Linear(50,50), nn.Dropout(0.1),\n",
    "            nn.Linear(50,50), nn.Dropout(0.1),\n",
    "            nn.Linear(50, output_dim)\n",
    "            )\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        rvec = torch.randn((len(batch),25))\n",
    "        \n",
    "        x_pre = x.narrow(1, 0,25)\n",
    "        \n",
    "        x = torch.cat((x_pre, rvec), dim = 1)\n",
    "        #print(x)\n",
    "        \n",
    "        for i in range(0,self.num_MP_layers):\n",
    "            #print(x.shape)\n",
    "            #print(x)\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, 0.25)\n",
    "            \n",
    "        x = tg_nn.global_max_pool(x, batch)\n",
    "        \n",
    "        x = self.MLPs(x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        for layers in self.children():\n",
    "            for layer in layers:\n",
    "                if hasattr(layer, 'reset_parameters'):\n",
    "                    layer.reset_parameters()\n",
    "    \n",
    "    def loss(self, pred, label):\n",
    "        #print('lossfuntion', F.sigmoid(pred[0]))\n",
    "        return F.binary_cross_entropy(F.sigmoid(pred), label.float().unsqueeze(1))\n",
    "    \n",
    "    \n",
    "    \n",
    "net_rni_adapted = Network_RNI_adapted()\n",
    "\n",
    "cross_validation(net_rni_adapted, py_graphs, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-copying",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
