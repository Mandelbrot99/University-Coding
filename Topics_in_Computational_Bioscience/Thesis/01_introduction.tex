\section{Introduction}
\subsection{Motivation}
From 2010 to 2020 the amount of data that was processed rose from 1.2 trillion gigabytes to 59 trillion gigabytes - an increase by 5,000\% \citep{data}. This exponential growth has evoked a high demand to leverage these amounts of data to promote scientific discoveries. In particular, it motivated the use of machine learning across all disciplines. Machine learning (ML) refers to a field of study that gives computers the ability to learn without being explicitly programmed. The benefits of this approach are immediate, since it allows computational systems to automatically process and analyse the enormous amounts of data, exceeding task-specific human capabilities considerably. 
Most recently, deep learning (DL) has emerged as a sub-discipline of machine learning denoting the use of multiple hidden layers in a network. Deep learning models can achieve even better accuracy than standard machine learning architectures if a substantially greater amount of data is available.

A field that has seen a particularly high interest in employing machine and deep learning technologies is drug discovery. Discovery and development of a new drug can take 12-15 years to end up with one approved drug requiring costs of more than \$1.3B. Only 2 out of 10 approved and marketed drugs can recover these costs \citep{hecht}. These figures put a great emphasis on making this process less resource-intensive promoting the use of machine learning. One of the main applications of ML for drug discovery lies in early stages that are concerned with target and hit identification as well as lead optimisation. Here, ML is used in  quantitative structure-activity relationships (QSAR) and quantitative structure-property relationships (QSPR) models to predict properties and activities of potential drug candidates. For instance, after finding a hit compound researchers would like to understand how its chemical structure can be optimised in order to improve properties like binding affinity, biological responses or physio-chemical properties \citep{LO20181538}. 

In abstract terms, fitting a QSAR/QSPR model amounts to finding a generally non-linear function between a class of molecules and a desired biological activity/property. ML methods solve this problem by learning this function from existing input/output pairs and almost any popular machine learning methods has been applied for QSAR analysis \cite{SHEN201929}. Popular examples include support vector machines \cite{supv1, supv2}, extreme gradient boosting \cite{XG1, XG2} and random forest \cite{RF1}. Since ML methods cannot operate on molecular structures directly, a suitable mathematical representation of molecules is needed. Finding and selecting this representation is referred to as featurization. It is crucial for the performance of ML methods that these features contain all of the information that impact the relationship between molecules and target property. Otherwise, the method may not be able to discern their true relationship. Hence, great emphasis has been put on developing methods for featurization to extract all of the relevant information.

\begin{figure}[h]
	\centering 
	\includegraphics[width=0.6\textwidth]{MPP_workflow.png}
	\caption{Illustration of the QSAR/QSPR workflow using ML/DL. Reprinted from \cite{yangMPP}. }
	\label{fig:mpp_workflow}
\end{figure}
Recently, Graph Neural Networks (GNNs) emerged as a class of deep learning methods and presented a novel solution to this problem \citep{duvenaud2015convolutional, li2019deepchemstable, STOKES2020688}. Until then, the reigning paradigm was to map molecules to a numerical vector in a fixed, pre-defined space that represents the presumably most important features of the molecule determining the target property. Expert knowledge is necessary to choose these features and the result of the prediction is ultimately biased by this knowledge or even incorrect if the feature selection was sub-optimal \citep{WIEDER2020}. GNNs solve this problem by automatising the selection process and learning the space itself to find the most suitable representation from a pool of given features for a specific prediction task. Figure \ref{fig:mpp_workflow} shows the branching between fixed and learned representations in a property prediction workflow. Not long ago, the potential of employing Graph Neural Networks in drug discovery was highlighted by the discovery of a new broad-spectrum bactericidal antibiotic `halicin' after decades of stagnation in the field. \cite{STOKES2020688} employed a directed-message passing neural network \cite{yangMPP} for both target selection to predict growth inhibitory effects against E. Coli. and ADME/T modelling predicting the toxicity of potential candidates.% This finding underlines both the potential of learned representations to lead to meaningful discoveries as well as their versatility to be employed at various stages of an early-stage drug discovery workflow. 

\subsection{Outline}
The goal of this thesis is to give an overview of GNNs in the context of learning the input representation of molecules for machine learning prediction tasks in drug discovery. We present their technical background together with an analysis of the results that have been obtained using GNNs. The outline for the thesis is as follows:
Firstly, we give a brief overview of other techniques that have been used for featurization. Most prominently, these concern molecular descriptors and fingerprints. Consecutively, the technical background to understand GNNs is presented. This involves a summary of molecular graphs building the basis for most molecular representations, a detailed explanation of circular fingerprints motivating the introduction of GNNs and ultimately an introduction to Message-Passing Neural Networks. These have been introduced as a general technical framework summarising some of the most prominent implementations of GNNs for drug discovery. In the next section we depict the application of GNNs in drug discovery by explaining two studies in detail. Finally, we discuss the advantages an disadvantages of GNNs and give an outlook for future research. 


%Two approaches to design these `features' have been introduced. On the one hand, fixed representations, like descriptors and fingerprints, map a molecule to a predefined vector space that contains the numerical values of some selected properties of a molecule. The drawback of this approach is that expert knowledge is necessary in order to make a meaningful selection of properties that are useful for learning the relationship to the target activity/property. Furthermore, this selection is inherently biased by the domain knowledge \citep{merkwirth}. The other, recently emerged, class of representations is given by learned representations. Instead of manually employing a mapping to a fixed space, Recurrent Neural Networks (RNNs) and Graph Neural Networks (GNNs) can be used to learn the space itself such that is contains the most suitable properties for predicting the target values.  The typical workflow of designing features and predicting an activity/property is summarised in figure \ref{fig:mpp_workflow}.



%A great potential of these developments in drug discovery lies within early stages to predict structure-activity relationships (SARs) and structure-property relationships (SPRs). These are grounded on the fundamental assumption that structurally similar molecules have similar activities/properties. For instance, after finding a hit compound in a drug screening campaign researchers would like to understand how its chemical structure can be optimised in order to improve properties like binding affinity, biological responses or physiochemical properties \citep{LO20181538}. Classically, this problem could only be solved through resource-intensive \emph{in vitro} screeening and \emph{in vivo} testing. Early quantitative structure-activity/property relationships (QSAR) models \citep{hansch}, that attempted to solve this problem \emph{in silico}, were limited by a lack of experimental data and the linearity assumption made for modeling \citep{LO20181538}.
%The introduction of high-throughput screening (HTS) and combinatorial synthesis resulted in a rapid explosion of the availability of data screening 100,000s or more samples per day for a desired biological activity. Ultimately, this led to the development of large databases containing the profiles of millions of chemical substances. How to effectively use these amounts of data for machine and deep learning methods has become a crucial challenge for drug discovery.


%There are numerous examples highlighting the potential of learned representations \citep{duvenaud2015convolutional, li2019deepchemstable, honda}. Most prominently, \cite{STOKES2020688} achieved a breakthrough in antibiotic discovery when they discovered a new broad-spectrum bactericidal antibiotic `halicin' after decades of stagnation in the field. They employed a directed-message passing neural network \cite{yangMPP} at two stages of their work. Firstly, this graph neural network was used to predict growth inhibitory effects against E. Coli. Later, another D-MPNN was used to predict the toxicity of potential candidates. This finding underlines both the potential of learned representations to lead to meaningful discoveries as well as their versatility to be employed at various stages of an early-stage drug discovery workflow. 
\subsection{Overview of methods for featurization}
A variety of methods have been used to design the input features for machine learning prediction tasks in drug discovery. Most of them fall into the category of fixed representations characterised by a pre-defined target space. These fixed representations can be broadly separated into molecular descriptors and fingerprints. Additionally, another form of learned representations has been proposed under the name of sequence models. Instead of operating on molecular graphs, they work with linear notations like SMILES \citep{smiles} or InCHI \citep{heller2015inchi}. These can be used to learn a representation by employing Recurrent Neural Networks (RNNs) or long short-term memory (LSTM) cells. Furthermore, \cite{honda} introduced a SMILES transformer to predict molecular properties. We will not discuss these sequence models in more detail in this report.
\label{sec:fixed_rep}
%Mapping molecules to a fixed representation is the classical form of generating a machine-interpretable input for QSAR/QSPR models. As the name suggests these are characterised by a fixed target space that contains a pre-selected choice of local or global information about the molecule.  Fixed representations can be broadly separated into two categories: Molecular descriptors and fingerprints. Descriptors are generally characterised by a more holistic representation of the molecule. Fingerprints, on the other hand, are local in nature by aggregating information of subgroups of atoms in a molecule.

\subsubsection*{Descriptors.}
As the name suggests, (numerical) descriptors represent molecules by describing some of their properties. According to \cite{todeschini2008handbook} `the molecular descriptor is the final result of a logical and mathematical procedure which transforms chemical information encoded within a symbolic representation of a molecule into a useful number or the result of some standardized experiment'. As this definition suggests, descriptors can be given by all kinds of properties that represent chemical information about the molecule. This makes them a straightforward, yet versatile means to encode a molecule mathematically. Critically, these properties need to be `useful'. Some basic requirements for the usefulness of a descriptor are outlined by \cite{Mauri2016} concerning for example
\begin{enumerate}
	\item invariance to node reorderings,
	\item invariance to rotations and translations of the molecule,
	\item definition by an unambiguous algorithm,
	\item well-defined applicability to molecular structures.
\end{enumerate}
However, these requirements ultimately depend on the application domain \citep{jiang}. This highlights a drawback of the descriptor approach since their usefulness as molecular representations for property prediction is constrained by  problem-specific knowledge \citep{SHEN201929}. 

Due to the enormous amount of different descriptors that have been proposed there are a lot of ways to categorise them. One attempt is based on the nature of the structural information that they require \citep{descript} and classifies them as constitutional, topological, geometric and quantum mechanical descriptors. Constitutional descriptors are the most rudimentary form of descriptors not taking into account any spatial information about the molecule. Quantum mechanical descriptors are among the most complex descriptors and their high computational requirements can make them unsuitable for large-scale screenings. 

Popular examples of descriptors for QSAR models include \begin{itemize}
	\item the Wiener index \citep{wiener1947structural, nikolic2001wiener}
	\item the coulomb matrix \citep{coulumb} or
	\item symmetry functions \citep{symfunc}.
\end{itemize}

%The expectations for the usefulness of a descriptor vary a lot depending on the application domain but according to \cite{Mauri2016} these typically include

%These desiderata are supposed to guarantee that the descriptor always gives the same representations for molecules that are considered the same and is generally applicable to all molecules. Beyond that, common extra requirements concern the inclusion of structural information (according to the fundamental principle of chemistry that different structures possess differeny properties), certain discriminative abilities and degeneracy/continuity, i.e. small structural differences result in small but existing differences in the value of the descriptor. 

%The variety of different descriptors that have been used for QSAR analysis is enormous and depends highly on the considered application.
%Any attempt to group descriptors into different categories would be quite arbitrary given the sheer amount of different application domains and descriptors. However, \cite{descript} propose an grouping based on the nature of the structural information that they require: 


\begin{comment}

Constitutional descriptors are the most rudimentary form of descriptors as they do not take into account any spatial information about the molecule but just its basic structural properties. Examples include basic attributes like the molecular weight the number of atoms but also more complex ones such as the sum of atomic van der Waals volumes. 

Topological descriptors are based on the connectivity of the atoms in a molecule and encode 2D structural properties using graph invariants of the underlying molecular graphs, i.e. properties that only depend on the abstract mathematical object and not on a particular labeling or ordering of the vertices. Such invariants include the Wiener index \cite{wiener1947structural, nikolic2001wiener} $W = \frac{1}{2} \sum_{i,j}^ N d_{ij}$, where $N$ is the number of non-hydrogen atoms and $d_{ij}$ is the edge count of the shortest part between atoms $i$ and $j$. A drawback of topological descriptors compared with constitutional descriptors is that they often tend to be less interpretable due to the abstract nature of the underlying graph. 

Geometric descriptors receive 3D information about the molecule as their input which may be resourceful to obtain from crystallographic data or molecular optimization \cite{Mauri2016}. However, they may also come with more information compared to descriptors that receive lower dimensional inputs. Therefore, they are usually employed in domains when this additional information is critical such as when two conformations are compared (TODO rewrite). An example of a geometric descriptor is given by the 3D Wiener Index which extends the 2D case by weighing the edges by their actual length or the gravitation index \cite{katritzky1996correlation}.

Finally, quantum mechanical descriptors are based on quantum mechanical calculations. For instance, they have been used to predict toxicity of molecules in QSAR studies \citep{REENU201589, senior2011qstr}. However, their tendency to require high computational costs impediment their application ot large scale virtual screenings.

Note that these categories are a non-exhaustive classification of descriptors and many others exist such as auto-correlation descriptors \citep{broto1984molecular} (TODO one more?). We conclude that descriptors are a popular method to represent molecules as they are a flexible means to encode the properties that are relevant to the particular application domain. However, this comes also with a downside as the performance of the application may heavily depend on the choice of descriptors and this selection is by no means a trivial task.
\end{comment}
\subsubsection*{Fingerprint Vectors.}
Descriptors are often derived from performing mathematical computations on the underlying structure and give a holistic representation of the substances considered. Fingerprint vectors on the other hand are given as bit vectors that indicate the presence or absence of a local property and are thus local in nature. Two classes of fingerprints can be distinguished \citep{SHEN201929}: Dictionary-based and hash-based fingerprints. Dictionary-based fingerprints such as Molecular ACCess System (MACCS) keys \citep{durant2002reoptimization} are computed by encoding each position of a vector as the presence or absence of a structural property from a pre-defined dictionary. However, these can be very sparse if arbitrarily large vectors are used leading to an inefficient representation. To overcome this sparsity, hash-based fingerprints have been introduced that employ a hashing algorithm to combine the different substructures into a unique bit-vector. These substructures can be enumerated linearly by iterating over all edge segments up to a given length in a molecular graph \citep{daylight} or in a circular manner as for extended connectivity fingerprints (ECFPs). 

ECFPs \citep{ECFP} are among the most popular fingerprints and they are often used as baseline results for the development of new featurizations techniques \citep{li2017learning, wu2018moleculenet, STOKES2020688}. Furthermore, they motivated the introduction of Graph Neural Networks that adapt their aggregation process to become differentiable. For this reason, we depict the technical details of ECFPs in section \ref{sec:circ_finger}.

Other circular fingerprints can be obtained from ECFPs by selecting different atom identifiers. This gives rise to fingerprints like FCFPs (Functional Class Fingerprints) that are based on the pharmacophore role of the atoms in a molecule (\cite{ECFP}), SCFPs \citep{SCFP} or LCFPs \citep{LCFP}. The choice of the identifier is ultimately responsible for the discriminative abilities of the fingerprint. Expert knowledge is needed to make a meaningful decision \citep{WIEDER2020}.

Like numerical descriptors, fingerprints are also a powerful means to represent molecules in form of a fixed-size vector. They differ from descriptors by implicitly encoding the local molecular structure. However, they suffer from a similar drawback as their usefulness for QSAR models is dependent on the choice of the atom identifier.

 \begin{comment}

\section{Intro}\label{sec:introduction}

Molecules form the smallest identifiable parts of covalent compounds that still retain their chemical properties \cite{molecules}. These covalent compounds can be found in all organisms, since together they form integral parts like proteins or the DNA making an understanding of molecules and their properties key to deciphering the foundations of life. Since molecules are complex physical entities in 3D space consisting of covalent bonds between atoms, identifying their chemical, physical or biological properties is by no means a simple task. \emph{Molecular property prediction} aims to characterise molecules according to their properties. In abstract terms this amounts to finding a nonlinear function from a class of molecules to a set of predefined properties.  Classically, \emph{in vitro} screeening and \emph{in vivo} testing were widely used in early stages of drug discovery in order to identify 'druggable' targets that display a desired biological response. Lead compounds were found by isolating natural products from microbiological fermentation, plant extracts and animal sources \cite{Gallop1994ApplicationsOC}. However, this process is extremely time and resource inefficient, because ... . More recently, \emph{in silico} methods attempt to embed the molecule into a mathematical representation which can then be used to learn this nonlinear relationship between the embedded molecules and their corresponding properties using statistical and machine learning methods. For instance, J. Stokes et al. achieved a huge breakthrough when the discovered the new antibiotics halicin \cite{STOKES2020688} after decades of stagnation in that field. Contrary to previous methods that translate molecules into a fixed predefined mathematical representation, they employed a Graph Neural Network that was able to learn a representation that then served as an input to an Artificial Neural Network to predict the target inhibitory effect against E. coli. %TODO: double check
Other classes of properties that have been of interest in the past are vast and comprise for example quantum-mechanic, physio-chemical, bio-physical or physiological properties \cite{wu2018moleculenet}. 

On October 5, 1981 a new version of the 'Fortune' magazine was release. Its cover page featured an article titled 'The   Next Industrial  Revolution:  designing  drugs  by  computer  at Merck' \cite{article}. This marked the begin of stage of naive euphoria in computational drug design with investments of millions of dollars in hardware and software. 
\subsection{Outline of the thesis}
The goal of this thesis is to investigate the role of Graph Neural Networks in the field of Molecular Property Prediction and its application to Drug Discovery being one of its best known representations in biology. In the rest of this section we will give a brief outline of the history of the dominant methods in MPP and elaborate on its role in Drug Discovery. Following that, we will introduce Graph Neural Networks and present the necessary theory behind them in order to understand their advantages and disadvantages. Then, we will compare the performance of GNNs to that of other methods in MPP and assess the benefit of Deep Learning in MPP in general. Finally, a summary of this thesis is given outlining its most important findings. 
\subsection{Disclaimer}
High level approach. Mathematically rigorous descirption can e found in \cite{KerberAdalbert2014Mcac}.




\subsection{Molecules}
Atoms are the smallest identifiable units of chemical elements which make up all matter in the universe. A fundamental principle of chemistry is that the atoms of different elements can combine to form chemical compounds and a lot of the study in chemistry is centered around understanding what happens when these compounds are formed. A chemical compound can be defined as a distinct group of atoms that are held together by chemical bonds (cite? Khan modelcules). Similar to the attraction between the positively charged nucleus and the negatively charged electrons that constitutes the structure of atoms, chemical bonds are caused by electrostatic attractions. While there is no clear separation between types of bonding from a physical perspective, it is still convenient to distinguish between different bonding types from a chemical perspective. The behaviour of the valence electron is the determining factor and this can be responsible for different properties of the resulting substance.

We are primarily concerned with two major types of bonds: Ionic bonds and covalent bonds.
In a simplified view, ionic bonding can be classified  as the transfer of a valence electron from one atom to the other resulting in the formation of two oppositely charged ions that hence attract each other and are bond together. Covalent bonding on the other hand is the result of electrostatic attraction between one or more electrons to the atomic nuclei of both atoms. This can be regarded as a sharing of the electrons across the two atoms. The structure resulting from covalent bonding is called a \emph{molecule}. These are of particular importance in biology making up the smallest identifiable parts of \emph{covalent compounds} that still retain their chemical properties \cite{molecules}. These covalent compounds can be found in all organisms, since together they form integral parts like proteins or the DNA making an understanding of molecules and their properties key to deciphering the foundations of life.


\subsection{Brief history of molecular representations}
In 1860 when the first International Chemical Congress was held in Karlsruhe, Germany, Alexander Butlerov predicted that determining the atomic arrangements of molecules would be the future of chemistry \citep{butlerov1861einiges}. He was the first person to use the word `structure' in its modern chemical meaning. This marked the birth of structural chemistry.\citep{wiswesser1968107}. Since then it took only seven year to develop the main ideas about line-formula conventions in familiar form like $$\chem{C_3H_7OH}.$$
No new practices appeared within 79 years until between 1947 and 1954 structure-delineating notations were introduced such as the Wiswesser line notation (WLN) which became very popular as it was easily interpretable by humans as well as computers. Compared to today's line formulae the WLN was very compact since memory efficiency was a critical factor in computers at that time.

When the advent of technology in the science accelerated in the 1980s, the role of chemical notations began do decline. \citep{Lawlor} attributes this to two main reasons. On the one hand, computer-manageable connection tables opened up new possibilities to experiment with structures. This meant that rather than working with the chemical formula itself, it was translated in a connection table where algorithms like similarity searches could be run to calculate compute properties, map reactions etc. 
The second reason is the increasing availability of graphics terminals. Multiple companies like Molecular Design Ltd. or CAS \citep{cas} introduced interactive services that enabled a translation between a graphical representation of compounds and their connections tables. Furthermore, this involved functionalities like searching by structure or substructure diagrams, which allowed chemists to perform the searching by themselves rather than being dependent on their information scientist intermediaries. Thus, a lot of popular representations that are still used today have shifted from prioritising their compactness to being specifically designed for computer applications \citep{smiles, heller2015inchi, cereto2015molecular}. Most prominently, the SMILES (Simplified Input Line Entry System) representation \citep{smiles} assigns a molecule a string of characters, where atoms are encoded by their atomic symbol and bonds are depicted by one of the following symbols: (-, =, \#, *, .). Furthermore, branches, rings and charge can be represented by the use of brackets numbers signs (+, -) making SMILES a versatile linear notation that is used to date. 

Nowadays, the use of molecular representations has branched. On the one hand, atom based representations like SMILES or InCHi are still present in chemical databases in order to uniquely identify a given molecule in a convenient language. These can be used in order to rebuild the molecule based on the representation \cite{molrep}. On the other hand, the advent of machine learning methods in all application domains demands a numerical representation of a molecule that represent its properties. Therefore another branch of representations is given descriptors that encode structural or chemical properties. Two established classes of descriptors are decsribed in detail  in section \ref{sec:fixed_rep}. 

 the reigning paradigm in molecular representations is given by fingerprint vectors, first introduced in , and descriptors. These methods have been experiencing particular popularity, because these representation can easily be used as the input for machine learning techniques for property prediction. However, this paradigm slowly begins to be challenged by newly emerging deep learning techniques such as Graph Neural Networks.
Rather than assigning molecules a fixed representation, these techniques aim to learn  that works.a flexible representation depending on the properties of interests of the molecules. This new approach seems equally innovative as crazy (TODO different word) and we will discuss the prospects of this in the following. 
\end{comment}
\section{Technical Background}
\subsection{Molecular Graphs}
\label{sec:mol_graphs}
Molecular graphs are a convenient means to represent molecules in two dimensions being the starting point for using various molecular representations such as circular fingerprints or GNNs. Formally a graph is defined as a tuple of sets $G = (V,E)$, where $V$ are the vertices of the graph and $E$ are the edges. Any edge $e \in E$ is uniquely identified by a pair of vertices $(v_1, v_2), \, v_1, v_2 \in V$ that it connects. In a molecular graph the vertices are given by the atoms and edges represent bonds between atoms. An example of a molecular graph is given in Figure \ref{fig:mol_graph}. We also note that the number of edges, i.e. the edge \emph{multiplicity}, may differ. This corresponds to the bond order in the molecule, i.e. the difference between the number of bonds and anti-bonds between two atoms, as introduced by \cite{pauling}. 

In computers, graphs are represented by a matrix - most commonly by their adjacency matrix $A$. The entries of this matrix are given by 
\begin{equation}
A_{ij} = 
\begin{cases}
1 & \text{if there is an edge from } v_i \text{ to } v_j \\
0 & \text{otherwise.}
\end{cases}
\end{equation}
%Compared to data structures like vectors, graphs are very high dimensional and irregular, simultaneously enabling the representation of more complex information and being harder to process.
Note that for an undirected graph, like a molecular graph, the adjacency matrix is always symmetric. In order to represent a graph by its adjacency matrix, we need to make a non-canonical choice of ordering the nodes. This is inconvenient for molecular graphs since these do not possess any kind of ordering and hence this representation is not well-defined. 

\begin{minipage}{0.5\textwidth}
	\centering
	\chemfig{OH\textsuperscript{1}-S\textsuperscript{2}(=[2]O\textsuperscript{3})(=[6]O\textsuperscript{4})-OH\textsuperscript{5}}
	\captionof{figure}{Molecular graph of sulfuric acid.}
	\label{fig:mol_graph}
\end{minipage}
\begin{minipage}{0.5\textwidth}
	\centering
	$
	\begin{blockarray}{cccccc}
	1 & 2 & 3 & 4 & 5 \\
	\begin{block}{(ccccc)c}
	0 & 1 & 0 & 0 & 0 & 1 \\
	1 & 0 & 1 & 1 & 1 & 2 \\
	0 & 1 & 0 & 0 & 0 & 3 \\
	0 & 1 & 0 & 0 & 0 & 4 \\
	0 & 1 & 0 & 0 & 0 & 5 \\
	\end{block}
	\end{blockarray}
	$
	\captionof{figure}{Adjacency matrix of the molecular graph representing sulfuric acid given the node ordering.}
	\label{fig:mol_adj_matrix}
\end{minipage}
\newline\newline
Figure \ref{fig:mol_adj_matrix} shows the adjacency matrix corresponding to the graph in Figure \ref{fig:mol_graph}. The ordering of the vertices is indicated by superscripts. If we assumed a different ordering of the vertices this would result in a permutation of the rows and columns of the adjacency matrix. For representation methods that use molecular graphs this means that the generated representation needs to be invariant to permutations of the adjacency matrix.
% This dependence on the node ordering is a fundamental problem for GNNs which is solved by the introduction of an \emph{inductive bias}. This means that  devising algorithms that give the same results regardless of a permutation of the matrix.

In order to represent information about molecules beyond the connection of its atom, the adjacency matrix is complemented with two more matrices - a node feature matrix and an edge feature matrix. These contain additional information about each atom and bond in a molecular graph. The node feature matrix has the same number of rows as the adjacency matrix, where row $i$ corresponds to the feature values for node $i$. The number of columns may vary depending on the number of features that are chosen to be encoded. An example for a node feature matrix is shown in Figure \ref{fig:mol_node_feature_matrix}. Finally, the edge feature matrix contains one row for every edge in the graph and the number of columns may vary depending on the number of encoded features, see Figure \ref{fig:mol_edge_feature_matrix}.

\begin{minipage}{0.45\textwidth}
	\centering
	$
	\begin{blockarray}{ccccc}
	O & S & 0H & 1H  \\
	\begin{block}{(cccc)c}
	1 & 0 & 0 & 1 &  1 \\
	0 & 1 & 1 & 0 & 2 \\
	1 & 0 & 1 & 0 &  3 \\
	1 & 0 & 1 & 0 &  4 \\
	1 & 0 & 0 & 1 &  5 \\
	\end{block}
	\end{blockarray}
	$
	\captionof{figure}{Example feature matrix of the graph in Figure \ref{fig:mol_graph}. The first two columns encode the atom type and the last two columns are a one-hot encoding of the number of implicit hydrogen atoms.}
	\label{fig:mol_node_feature_matrix}
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
	\vspace{.1cm}
	\centering
	$
	\begin{blockarray}{cccc}
	1 & 2 & 3  \\
	\begin{block}{(ccc)c}
	1 & 0 & 0 &  (1,2) \\
	0 & 1 & 0 &  (2,3) \\
	0 & 1 & 0 &  (2,4) \\
	1 & 0 & 0 &  (2,5) \\
	\end{block}
	\end{blockarray}
	$
	\captionof{figure}{Example edge feature matrix of the graph in Figure \ref{fig:mol_graph}. The chosen features represent a one-hot encoding of the bond type.}
	\label{fig:mol_edge_feature_matrix}
\end{minipage}
\newline\newline
%TODO : more sources on moelcular graphs 
While the graphical representation allows for the representation of complex 3D information of molecules, there are some drawbacks of working directly on the graph level. Firstly, not all molecules can be represented as graphs \citep{molrep} such as those that contain bonds that cannot be explained by valence bond theory. Secondly, graphs are not a suitable means of depicting molecules whose arrangement of molecules change over time as this would require a reordering of the adjacency matrix every time. Finally, graphs are neither very compact nor easy to process. The adjacency matrix alone has a memory requirement quadratic in the number of atoms in the molecule and depending on the amount of atomic and bond information that is to be encoded the feature matrices might get even bigger. As opposed to this, a linear representation as a single string allows for using substantially less memory while being simultaneously easier to store and process by algorithms. Therefore, graphs are usually used as the basis of more compact representations that we are going to depict in the following subsections. 
\subsection{Extended-Connectivity Fingerprints}
\label{sec:circ_finger}
%Circular fingerprints are among the most widely used fingerprints for QSAR models and inspired the development of GNNs. 
%Since most of the recent studies that explore fixed and learned representations for drug discovery use ECFPs for baseline results \citep{li2017learning, STOKES2020688,wu2018moleculenet}, we choose to demonstrate their technical details in the following.

%Extended Connectivity Fingerprints (ECFPs) were first introduced by the software Pipeline Pilot in 2000 and then described in detail by \cite{ECFP}. The origin of this representation goes back to \cite{morgan} who introduced the Morgan algorithm on which ECFPs are based. This is why they are also often referred to as Morgan fingerprints. This algorithm assigns numerical values to each atom by an iterative process that does not depend on a specific numbering of the atoms. It is depicted in Algorithm \ref{algo:morgan}.
Extended-Connectivity fingerprints belong to the class of circular fingerprints and are based on a variation of the Morgan algorithm \citep{morgan} which is outlined in Algorithm \ref{algo:morgan}. Given a molecular graph, they assign each atom a unique identifier that is based on a selection of properties. Then, this information is propagated from each atom to its neighbours. Contrary to the original Morgan algorithm, this process is terminated after a pre-defined number of iterations. In the following we detail the generation of ECFPs.

%ECFPs adapt this algorithm by stopping the while-loop after a predefined number of steps rather than until completion and storing the intermediate values. We outline each part of the full algorithm in detail in the following paragraphs.

Firstly, every non-hydrogen atom is assigned an integer identifier that can be chosen arbitrarily as long as it is independent of the node ordering, e.g. the atom's mass or atomic number. \cite{ECFP} choose a 32 bit integer value as an identifier that results from hashing the properties used in the Daylight atomic invariants rule \citep{smiles2}. A set $A$ is created containing the initial identifiers of all the atoms. Then, for each atom we add the atom's own identifier and that of its immediate neighbouring atoms together with their bond order to an array (ordered by the atoms' identifiers and the order of the attaching bonds). These values are then hashed to get a single-integer identifier which overrides the initial identifier that the atom was assigned. This way, each atom updates its own features by incorporating those of its neighbours. The updated identifiers are added to the set $A$ if there are no two structurally equal identifiers in the set.

Two identifiers are considered structurally equal if they encode the same substructure of the molecule after an equal number of iterations . This may occur for example for the nitrogen and oxygen atoms at the top and right of the structure shown in Figure \ref{fig:iteration_ECFP}. After two iterations they both encode the same substructure consisting of the two carbon atoms, the oxygen atom and the nitrogen atom. To avoid this information redundancy only one of the corresponding hashes is added.
\begin{algorithm}[t]
	\SetAlgoLined
	\KwData{Molecular graph}
	\KwResult{Unique node ordering}
	Assign each atom its number of non-hydrogen neighbours as initial value\;
	\While{not done}{
		\For{atom in atoms}{
			Update value by the sum of the values from the neighbouring atoms\;
			
		}
		\If{number of different values does not change }{
			break\;
		}
	}
	\caption{Morgan Algorithm}
	\label{algo:morgan}
\end{algorithm}
\begin{figure}[h]
	\centering 
	\includegraphics[width=0.5\textwidth]{iterative_updating.png}
	\caption{Illustration of the iterative updating in the computation of the ECFPs. In this example the atom type is used as an identifier. In iteration 0 the middle atom' identifier only represents the information about its own type. After the first iteration it has aggregated the information from its immediate neighbors and after the second iteration the represented substructure has grown even further. Reprinted from \cite{ECFP}. }
	\label{fig:iteration_ECFP}
\end{figure}
The first step is repeated $n$ times using the updated identifiers of each atom as the the initial identifiers for the next step. This way, the identifier of each atom represents substructures of increasing sizes as illustrated in Figure \ref{fig:iteration_ECFP}. After the completion of the $n$ steps, numerically equal values are removed from the set $A$ and the remaining identifiers define the circular fingerprint. This final set of identifiers can be interpreted in a similar fashion as dictionary-based fingerprints. Each identifier in $A$ corresponds to a bit in a huge virtual bit string denoting the presence or absence of a particular substructural feature \citep{ecfp_bit}. This representation allows for the folding of the string into a bit vector of consistent size, e.g. 1024 bits. 


%We clearly see ECFP's local nature. It generates a representation of the entire molecule by using only local operations thereby implicitly encoding the molecule's structure. 
\begin{figure}[h]
	\centering 
	\includegraphics[width=0.5\textwidth]{test1.png}
	\caption{Molecular graphs obtained using the code in Appendix \ref{sec:a_sim}.}
	\label{fig:molsa}
\end{figure}
\begin{figure}[h]
	\centering 
	\includegraphics[width=0.5\textwidth]{test2.png}
	\caption{Molecular graphs obtained using the code in Appendix \ref{sec:a_sim}.}
	\label{fig:molsb}
\end{figure}
To better understand the aggregation scheme we compare the predicted similarity scores obtained using two pairs of molecules in Figure \ref{fig:molsa} and \ref{fig:molsb} for $n=1,\ldots,5$.
%In the literature ECFP fingerprints are usually used with $n=2$ which is referred to as ECFP4 (4 being the maximum diameter of substructures considered). To understand the importance of this parameter,  we compare the predicted structural similarity of two pairs of molecules in Figure \ref{fig:molsa} and \ref{fig:molsb} for $n=1,2,3$. 
%Furthermore, we list the similarity scores of Atom-Pair fingerprints \citep{atompairs} that are considered more suitable for representing larger molecules since they aggregate information of all pairs of atoms seperated by an arbitrary distance. 
The results are described in Table \ref{tab:dis_metric_2Dshapes}. Note that we adopt the notation used in the literature where the number behind `ECFP' denotes the maximum diameter of substructures considered. For instance, ECFP4 corresponds to $n=2$. The source code for this experiment can be found in the Appendix \ref{sec:a_sim}. We choose a pair of smaller molecules and one of larger molecules to understand if their size has any impact on the similarity scores. As we expand the size of the substructure that each atom's identifier represents, more dissimilarities between the molecules are discerned as expressed by a drop in the scores. However, the scores decrease more slowly with an increasing $n$ until they eventually plateau. This is because no new distinct substructures are identified after a certain number of iterations. This corresponds to no new structurally unique identifiers being generated in the computation of the fingerprint as outlined above. We conclude that $n$ can be regarded as parameter that determines how precisely two molecules are distinguished. In the literature, ECFP4 fingerprints are the common choice.
%  Eventually, the scores stabilise. The drop in the similarity scores is substantially more significant for molecules a \& b.  This may be because for the first pair the proportion of dissimilar parts is larger for greater $n$ relative to the second pair. This underlines the idea of circular fingerprints 
%We remark that this hyperparameter appears to play an important role for when ECFPs are used as the input features for machine learning techniques. We can interpret $n$ as a regularisation parameter that penalises structurally too dissimilar molecules to be assigned too similar properties by a machine learning algorithm.
\begin{table}[h]
	\centering
	\begin{tabularx}{0.69\textwidth}{l
			r
			r 
			r
			r
			r
		}
		\toprule
		
		\bf{Molecules}  &  \multicolumn{1}{c}{\bf{ECFP2}}   & \multicolumn{1}{c}{\bf{ECFP4}}&  \multicolumn{1}{c}{\bf{ECFP6}} & \multicolumn{1}{c}{\bf{ECFP8}} &\multicolumn{1}{c}{\bf{ECFP10}} \\
		\midrule
		a \& b   & 56.25\%   &  46.15\%  & 34.29\%   & 32.43\% & 32.43\%  \\
		
		c \& d   & 68.66\%  &  58.71\% &  52.86\%   & 46.32\% & 43.09\%  \\
		
		
		\bottomrule
	\end{tabularx} 
	
	
	\caption{Sørensen-Dice similarity values \citep{sorensen1948method, dice1945measures} using different fingerprints for molecules in Figure \ref{fig:molsa} and Figure \ref{fig:molsb} respectively}
	\label{tab:dis_metric_2Dshapes}
\end{table}
\begin{comment}
\begin{table}[h]
	\centering
	\begin{tabularx}{0.57\textwidth}{l
			r
			r 
			r
			r
		}
		\toprule
		& \multicolumn{3}{c}{Morgan Fingerprints} &   \\
		\cmidrule(r){2-4} 
		\bf{Molecules}  &  \multicolumn{1}{c}{\bf{ECFP2}}   & \multicolumn{1}{c}{\bf{ECFP4}}&  \multicolumn{1}{c}{\bf{ECFP6}} & \multicolumn{1}{c}{\bf{APFP}} \\
		\midrule
		a \& b   & 56.25\%   &  46.15\%  & 34.29\%    & 50.88\%   \\
		
		c \& d   & 68.66\%  &  58.71\% &  52.86\%  & 54.47\% \\
		
		
		\bottomrule
	\end{tabularx} 
	
	
	\caption{Sørensen-Dice similarity values \cite{sorensen1948method, dice1945measures} using different fingerprints for molecules in Figure \ref{fig:molsa} and Figure \ref{fig:molsb} respectively}
	\label{tab:dis_metric_2Dshapes}
\end{table}
\end{comment}



\subsection{Message Passing Neural Networks}
\label{sec:MPNN}

Convolutional Neural Networks \citep{lecun1999object} have achieved remarkable success at learning representations of grid-like structures such as images. The idea to generalise these frameworks to less regular structures like graphs motivated the introduction of many Graph Convolutional Neural Networks (GCNNs) as in \citep{li2015gated,duvenaud2015convolutional,Kearnes_2016, Sch_tt_2017}. An attempt to unify all these approaches in a general framework was made by \cite{GilmerSRVD17} introducing Message Passing Neural Networks (MPNNs). In the following we will outline how MPNNs work and mention how specific GNNs can be restored. 

MPNNs combine edge and node properties of a graph together with an implicit encoding of the structure. This is achieved through a similar aggregation step as for circular fingerprints in which a node updates its own feature vector by combining it with the aggregated information from its neighbours. The difference is that a weighting of the features can be learned which allows for emphasising more important features while less important ones contribute less to the final representation. As an input MPNNs require a graph represented by its adjacency matrix and the node and edge feature matrices that encode the properties. They output a feature vector for the full graph. 

An entire forward pass of an MPNN can be divided into two phases: The message passing phase that runs for $T$ time steps and a consecutive readout phase. Each node stores information about its own features and those of its local environment in a hidden state vector ${\vecb h}_v^t \in {\mathbb{R}}^L$.  $\vecb h_v^0$ is initialised with the node's feature vector $\vecb x_v$ obtained from the node feature matrix. For each time step during the first phase any node receives `messages' about its neighbours' hidden states and then updates its own hidden state based on that. Specifically, this can be described by the two equations

\begin{eqnarray}
\vecb m_v^{t+1} & =& \sum_{w\in N(v)} M_t(\vecb h_v^t, \vecb h_w^t, \vecb e_{vw}) \label{eq:message_passing} \\
\vecb h_v^{t+1} & =& U_t(\vecb h_v^t, \vecb m_v^{t+1})\label{eq:updating}
\end{eqnarray}
where $\vecb m_v^t$ is the `message' node $v$ receives at time $t$ which is composed of the sum of the message functions $M_t$ from its immediate neighbours that can depend on their own hidden state $\vecb h_v^t$, the neighbour's hidden state $\vecb h_w^t$ and features of the edge connecting them.

After $T$ time steps, any node $v$ has now received information about any node $w$ that are at most $T$ edges away. This is because after the first step $w$'s neighbors receive information about $w$'s hidden state which is in turn incorporated in their own hidden state. In the next iteration, $w$'s neighbours pass their hidden state, incorporating information about $w$'s hidden state, to their own neighbours. This way, information about $w$'s hidden state is propagated through the graph and after $T$ iterations, $v$ receives this information. This idea is illustrated in Figure \ref{fig:mpnn}.
\begin{figure}[h]
	\centering 
	\includegraphics[width=0.7\textwidth]{MPNN.png}
	\caption{Illustration of the message passing in a MPNN. Reprinted from \cite{mpnn_graphics}. }
	\label{fig:mpnn}
\end{figure}
 
The consecutive readout phase now computes a feature vector for the whole graph as given in equation \ref{eq:readout}

\begin{equation}
\hat{\vecb y} = R(\vecb h_1^T, \ldots, \vecb h_{|V|}^T) \label{eq:readout}
\end{equation}
Different choices for the functions $M_t, U_t$ and $R$ restore different Graph Neural Networks proposed in the literature. All of them have in common that they are differentiable and learned through backpropagation. Furthermore, $R$ must be permutation-invariant in order for the MPNN to be insensitive to the node ordering.

Note the parallels to the computation of circular fingerprints as outlined in section \ref{sec:circ_finger}. This is no coincidence as the introduction of one of the earliest convolution-based GNNs \citep{duvenaud2015convolutional} was motivated by replacing the steps in the computation of circular fingerprints by differentiable analogs. This will be explained in detail in section \ref{sec:conv}.


\section{Results}
\label{sec:results}
In this section we study the application of GNNs to drug discovery. Their main motivation lies in overcoming the dependence on an explicit choice of features that are used to compute a fixed representation. For descriptors this choice is given by the selection of properties to be represented by the descriptor. Molecular fingerprints require this selection in form of the identifier that is used to initialise the atom's values. This manual feature design means that a bias is imposed and the resulting method can only perform as well as the feature selection allows \citep{merkwirth}. GNNs remedy this problem by using deep learning to learn the features most relevant to a property of interest.

We present two applications of GNNs to drug discovery that highlight their potential as state-of-the-art featurization techniques. Anticipated benefits were listed by \citep{SHEN201929} and comprise:
\begin{enumerate}
	\item a compact final representation of the molecule, 
	\item enhanced interpretability, 
	\item the possibility to use attention algorithms that allow the model to focus on the most relevant parts of the molecule \citep{deepchemstable, graphattentionmpp}
	\item an improvement in predictive performance given large enough data sets \citep{yangMPP}.
\end{enumerate}
We will review these factors in the next section to understand and discuss their validity based on the presented results in this section.

\subsection{GNNs for the prediction of solubility, drug efficacy and photovoltaic effciency}
\label{sec:conv}
\subsubsection*{Setup.}
The method presented by \cite{duvenaud2015convolutional} was one of the first to challenge the state-of-the art approach of using circular fingerprints for the prediction of molecular properties. They noticed that the mechanism used for circular fingerprints, i.e. applying the same operation locally everywhere, was analogous to that of convolutional neural networks. This motivated the idea of creating a differentiable `neural graph' fingerprint that could be learned through backpropagation. To implement this, they went ahead to replace every non-differentiable operation of circular fingerprints by a differentiable analog. These adaptations are illustrated by a comparison of both algorithms in Figure \ref{fig:algos}.
Note that invariance to the node ordering is achieved by using a permutation-invariant aggregation function, i.e. summing.
\begin{figure}[h]
	\centering 
	\includegraphics[width=\textwidth]{algos.png}
	\caption{Comparison of the algorithm that generated circular fingerprints  with that for generating neural graph fingerprints. Note that the left algorithm uses the interpretation of the atom's hashed identifiers as indices of bits in an array as explained in section \ref{sec:circ_finger}. Reprinted from \cite{duvenaud2015convolutional}. }
	\label{fig:algos}
\end{figure}

This method for generating neural graph fingerprints is summarised by the message-passing framework presented in section \ref{sec:MPNN} using the following message- and readout functions:

The message function $M_t$ is the same across all time steps and given by
\begin{align*}
M(\vecb h_v, \vecb h_w, \vecb e_{vw}) &= \frac{1}{|N(v)|}\vecb h_v + \vecb h_w%(\vecb h_w, \vecb e_{vw}), 
\intertext{%where $(\cdot, \cdot)$ denotes concatentation. 
	The update and readout functions are given by}
U_t(\vecb h_v^t, \vecb m_v^{t+1}) & = \sigma( \vecb m_v^{t+1} \vecb H_t^{\text{deg}(v)})
\intertext{which includes learnable parameters as given by the matrices $\vecb H_t^{k}$ for all time steps $t$ and node degrees $k$. $\sigma$ denotes the sigmoid activation function. Finally, the readout function is given by}
 R(\vecb h_1^T, \ldots, \vecb h_{|V|}^T) &= \sum_{v,t} \text{softmax}( \vecb h_v^t \vecb W_t)
\end{align*}
with learnable matrices $\vecb W_t$ for all time steps $t$. 

\subsubsection*{Experiments} 
\cite{duvenaud2015convolutional} applied their proposed architecture to predict the following properties of molecules:
\begin{itemize}
	\item Aqueous solubility as in \citep{delaney2004esol},
	\item efficacy as a drug against a parasite that causes malaria as measured by \cite{gamo2010thousands},
	\item photovoltaic efficiency as in \citep{hachmann2011harvard}.
\end{itemize}
To obtain the predicted property from the output of the GNN they used a fully connected neural network that receives the output of the GNN and computes the predicted value of the property. This whole architecture is trained in an end-to-end fashion through backpropagation such that the GNN can adapt its weights to favour the contribution of the most relevant features for the desired property. Figure \ref{fig:table_du} reports the RMSEs of both the GNN and ECFPs as input features for the neural network. 

\begin{figure}[h]
	\centering 
	\includegraphics[width=\textwidth]{table_du.png}
	\caption{Comparison of the root-mean-square error on the three data set mentioned above for circular fingerprints and neural fingerprints (GNN). Reprinted from \cite{duvenaud2015convolutional}. }
	\label{fig:table_du}
\end{figure}
Overall, we see that the GNN gives slightly better results than ECFPs. However, the advantage of GNNs is not consistent across all data sets. For predicting drug efficacy the combination of using ECFPs with linear layers in the fully connected network achieves the best scores. However, GNNs perform only slightly worse. These results presented a first promising indicator of the potential of GNNs for molecular property prediction and motivated further research in this area.

\begin{comment}
\subsubsection{Deep Tensor Neural Networks}
This archtiecture refers to the one porposed by \cite{Sch_tt_2017}. Here, the message function at time $t$ is given by 
\begin{align*}
M_t(\vecb h_v, \vecb h_w, \vecb e_{vw}) &= \tanh\left(\vecb W^{fc}((\vecb W^{cf} \vecb h_w^t + \vecb b_1)\odot (\vecb W^{df} \vecb e_{vw} + \vecb b_2 ))\right), 
\intertext{where $\odot$ denotes the element-wise product, $\vecb W^{fc},\vecb W^{cf}, \vecb W^{df}$ are learnable matrices and $\vecb b_1, \vecb b_2$ are learnable biases. The update function at time $t$ is given by:}
U_t(\vecb h_v^t, \vecb m_v^{t+1}) & = \vecb h_v^t \ \vecb m_v^{t+1}
\intertext{Finally, the readout passes each final hidden state individually through a single hidden layer neural netork and sums the results:}
R(\vecb h_1^T, \ldots, \vecb h_{|V|}^T) &=\sum_v \text{NN}(\vecb h_v^t)
\end{align*}
\end{comment}
%\subsection{Directed MPP ? \cite{yangMPP}}
\begin{comment}
\subsection{Graph Convolutional Neural Networks}
\label{sec:GCN}
These belong to the most popular classes of Graph Neural Networks and was proposed by \citep{gcn}. A detailed derivation of the message and update functions can be found on \cite{GilmerSRVD17}. The resulting functions are given by:

\begin{align*}
M_t(\vecb h_v^{t}, \vecb h_w^{t}) &= \sum_{w \in N(v) \cup \{v\}}(\deg(v)\deg(w))^{-1/2} \vecb h_w^t
\intertext{The update function at time $t$ is given by:}
U_t(\vecb h_v^t, \vecb m_v^{t+1}) & = \sigma (\vecb W^t \vecb m^{t+1})
\end{align*}
with a trainable matrix $\vecb W^t$ and a non-linear activation function $\sigma$, e.g. ReLU. This can be thought of as a generalisation of the MPNN framework that includes self-loops in the message passing step.

\subsubsection{Graph Attention Networks}
Graph Attention Networks proposed by \cite{gan} extend GCNs by replacing the normalisation constants $\deg(v)\deg(w)$ in the aggregation step by a learned attention score
\begin{align*}
M_t(\vecb h_v^{t}, \vecb h_w^{t}) &= \sum_{w \in N(v)}\alpha^t_{vw} \vecb h_w^t.
\end{align*}
The attention score weighes the information from neighbours according to how important they are. Details about how $\alpha_{vw}^t$ is computed can be retrieved from the paper \citep{gan}.
\subsubsection{Attentive FP}
The most recent state of the art Graph Neural Network architecture was propsed by \cite{attentivefp}. It proceeds similarly to GANs by stacking multiple attention layers together with Gated Recurrent Units (GRUs) to update the nodes' hidden states recursively and allow an atom to focus on its most important neighbors. To generate the final graph embedding, Attentive FP treats the whole molecule as a virtual node that connects to all its atoms. Then, it employs the same architecture as for the individual atoms to learn the molecule's final representation. 
These 
GCN, GAN, Attentive FP?
\end{comment}

%\subsection{Sequence modeling}
%SMILES RNNs, LSTMs? \cite{honda}
\subsection{GNNs for antibiotic discovery}
The second application of GNNs for drug discovery is concerned with antibiotic discovery. The discovery of new antibiotics is becoming increasingly difficult due to the dereplication problem that the same molecules are discovered over and over \citep{COX201798}. Given the simultaneous stagnation of the success of existing methods for antibiotic discovery and development of antibiotic-resistant determinants this creates a great urge for new methods to enter the stage.  

\cite{STOKES2020688} employed a GNN for both target identification and the prediction of toxicity of potential antibiotic candidates. The molecular representation was built using a directed-message passing neural network \cite{yangMPP}. This D-MPNN extends the framework from section \ref{sec:MPNN} by considering directed messages associated with bonds instead of atoms. This is motivated by the hope to avoid messages being passed forth and back between two atoms which can lead to noise in the representation. We refer to \cite{yangMPP} for more details.

The full workflow can be separated into three stages. The fist stage concerns the training of the model and a classifier.
Since (D-)MPNNs can struggle to represent global features of molecules, especially if the number of message passing iterations is greater than the longest path in the molecule as discussed in section \ref{sec:MPNN}. Therefore, the final representation generated by the D-MPNN was augmented with 300 additional molecule-level features. This combined representation was then input in a feed-forward neural network that outputs a number between 0 and 1 as the prediction of the molecule showing growth inhibitory against E. Coli. This whole architecture is trained in an end-to-end fashion such that the D-MPNN can generate a representation that is highly attuned to the desired property.
The training of this architecture was performed using a set of 2335 molecules that had been classified as hit or non-hit using 80 \% growth inhibition against E. coli BW251113 \cite{ZAMPIERI20171214} as a hit cut-off. On the test data this model achieved an AUC-ROC score of 0.896.

In the second stage, 20 folds of the trained model using different weight initialisations were applied to 6,111 molecules from the Drug Repurposing Hub \citep{corsello} to predict their probability of growth inhibition against E. Coli. The 20 different results were averaged to arrive at the final prediction scores. 

Finally, the best scoring 99 molecules were empirically tested for growth inhibition out of which 51 displayed this property. The resulting 51 molecules were ranked according to their clinical phase of investigation, structural similarity to the training data set and their toxicity that was also predicted using a D-MPNN. This resulted in the discovery ot the broad-spectrum bactericidal antibiotic halicin with a very low structural similarity to its nearest neighbour antibiotic in the training data emphasising the model's capacity to generalise. 

This case study highlights the versatility and potential of using Graph Neural Network for property prediction in early drug discovery. They could be employed for both prediction of growth inhibitory effects as well as toxicity and resulted in the finding of a new antibiotic after years of stagnation in this field. \cite{STOKES2020688} also reported the prediction scores using a combination of Morgan fingerprints with various classifiers. The rank of the newly discovered antibiotic halicin was lower in all of them ranging between 773-2644 compared to 69 for the D-MPNN approach. Therefore, it could be argued that halicin would not have been found if molecular fingerprints had been used. However, there is still some correlation among the top scoring molecules. For instance, both the D-MPNN and Morgan fingerprints predict the same highest ranking molecule and the fourth place for D-MPNN is in second place for Morgan fingerprints. The question that remains to be answered is if this is just a correlation of numerical values and halicin being ranked much higher for learned representations is just a fortunate coincidence or if the predictions of GNNs actually carry more physical relevance.

Despite this breakthrough using the GNN approach, \cite{STOKES2020688} still emphasise the importance of a combination of \emph{in silico} and empirical investigations. 

\section{Discussion}
Artificial intelligence and machine learning are currently one of the most rapidly evolving research areas and the progress in these fields has direct impacts on  a great variety of disciplines. Recently, a variety of Graph Neural Networks has been introduced as a way to automatise the feature selection for molecular property prediction. Instead of relying on expert knowledge to select the most relevant attributes to be used for a computer-interpretable interpretation, which has been shown to heavily impact the performance of the property prediction \citep{tian}, Graph Neural Networks manage to learn a continuous vector representation that is highly attuned to the property of concern. After reviewing two applications of GNNs in drug discovery we come back to the anticipated benefits oultined in section \ref{sec:results}. 

While GNNs do learn a compact representation compared to molecular fingerprints if they are stored as sparse bit-vectors, this is at the downside of substantially higher computational costs. Molecular fingerprints as well as most descriptors can be computed immediately and serve as an off-the-shelf representation of molecules. The higher computational costs of GNNs is not only attributed to the time that it takes to train the network and adapt the weight. Beyond that, they require resource-intensive hyperparameter tuning in order to find a suitable configuration of weights. 

In terms of interpretability, the comparison of fixed representations and GNNs is more level. Methods like studying the activations of the GNN can be used in order to identify the substructures that maximally excite certain feature maps in the GNN as described by \cite{duvenaud2015convolutional}. However, these methods ultimately do not help to understand the contribution of the initialised features to the final representations. For molecular fingerprints on the other side, \cite{jiang} show that the SHAP method \citep{lundberg2017unified} allows for a way to interpret the final prediction scores by computing the contribution of each input feature that was selected. This might be even more helpful than understanding the correspondence between substructures and feature maps in a GNN. 

As another anticipated benefit the is use of attention algorithms mentioned in section \ref{sec:results}. While attention algorithms have been introduced as an extension for GNNs \citep{gan, graphattentionmpp}, the ultimate benefit of GNNs is more fundamental. GNNs are extremely flexible and can be extended very easily. They are a current topic of research even beyond the prediction of chemical properties and general advances of the framework can have immediate impacts on their application in drug discovery. This makes them promising to take over the paradigm in molecular representations as their advances are much more far reaching compared to the development of a new descriptor.

Finally, we compare the predictive accuracy of GNNs to that of molecular fingerprints and descriptors. While many studies report that learned representations are superior to fixed representations in terms of the property prediction accuracy for a variety of different applications \citep{wu2018moleculenet,yangMPP, korolev}, there is still no consensus on this. Others report the dominance of descriptor-based approaches and fingerprints \citep{mayr, jiang}. This suggests that there are other relevant factors that influence which approach is better. Since there are substantially more parameters involved in learning a representation compared with using a fixed representation, a sufficiently large data set is critical to learned approaches. Something else to take into account is the mode of evaluation. As mentioned by \cite{SHEN201929}, the evaluation of model performance is critical and non-trivial to molecular property prediction. This is because unlike images there is no standard to generating ground truth labels for the data. These are usually obtained from experiments and experimental procedures can differ and are subject to human errors. Furthermore, baseline models are often not tuned enough to reach peak performance. Therefore, the question of which method is the state of the art remains to be answered. However, the ongoing research on GNNs is likely to further increase their performance, ultimately leaving fixed representations behind.
%( Finally a fundamental assumption of employing and comparing machine different machine learning models is that training and test data are all independently identically distributed. It has been noted that for different molecules this requirement is very hard to verify let alone achieve.) (find source)


I personally think that learned representation obtained by GNNs will be the future of molecular property prediction in drug discovery. While their computational costs are a considerable drawback these are likely to become less significant as GPUs continue becoming faster and more widely available. Furthermore, GNNs are able to achieve state-of-the-art performance and they have already been successfully used to impel areas that were stagnating before their introduction \citep{STOKES2020688}. While there are still publications reporting better results for descriptor-based approaches, GNNs' great potential for adjustment helps them to continually improve. For example, since the message passing approach may struggle to represent global properties of a graph, a global readout has been suggested \citep{barcelo2019logical} to overcome this problem. Finally, GNNs enable their application to property prediction without having to rely on domain experts that need to select appropriate features. This allows for a wider application across disciplines making GNNs a versatile and promising tool for the future. 

In my opinion, a key challenge for the future of GNNs is to enable their employment without the necessity to tune a lot of hyperparameters. This will allow for their use across even more disciplines without needing computer scientists as a guide. This could be achieved by conducting more studies that concern multiple data sets to obtain a better understanding for the role of individual parameters.  
\section{Conclusion}
In this report we have studied the role of GNNs for generating molecular representations that can be used as an input for machine learning methods predicting molecular properties. These turned out to be an extension of molecular fingerprints by making their aggregation differentiable and learning a weighting of the atom identifiers according to their relevance for the target property.
Two applications of GNNs for drug discovery were explained in detail. The first by \cite{duvenaud2015convolutional} concerned the prediction of molecular properties like solubility or their efficacy as a drug. The second application by \cite{STOKES2020688} was about the application of GNNs in the process of finding a new antibiotic. 
Finally, we compared learned representations with fixed representations in terms of accuracy, computational costs and interpretability. Despite fixed approaches outperforming GNNs in terms of their computational costs, we hypothesised Graph Neural Networks to be a key future technology for molecular property predictions. This is due to their performance that is likely to be further improved in the line of ongoing research and due to their wide applicability given that they do not require expert knowledge to be used. 