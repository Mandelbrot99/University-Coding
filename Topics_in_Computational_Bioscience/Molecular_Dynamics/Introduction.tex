\section{Summary}
A substantial drawback of Molecular Dynamics simulations is the trade-off between accuracy and efficiency. 
\emph{Ab initio} molecular dynamics (AIMD) yields the most accurate results but is limited to space- and time scales too small for most applications. On the other hand, the deployment of empirical force fields enables the study of larger scales but is approximative in nature and therefore lacking the accuracy of AIMD. 
\newline\newline
In their paper \cite{PhysRevLett.120.143001} L. Zhang et al. present the Deep Potential Molecular Dynamics (DeePMD) method for molecular simulations addressing this trade-off. This method leverages Machine Learning (ML) to learn the functional dependence of the potential on the atomic configurations and consolidate the benefits of the two approaches. Contrary to former ML approaches, DeePMD does not make use of auxiliary quantities like symmetry functions or the Coulomb matrix in order to preserve the symmetry of the system under translations, rotations and permutations. Instead, it makes use of the extensive character of the potential energy to assign each atom a local reference frame contributing to one summand $E_i$ of the potential energy $E = \sum_i E_i$. The scheme to find $E_i$ can be split up into two steps. 
\begin{enumerate}
	\item In this first step, the inputs for the neural network (NN) are generated. This is done by setting up a coordinate system for each atom $i$ with the atom as its center. Then, for each neighbor atom $j$ within a preset radius $R_c$, the reciprocal distances\footnote{The reciprocal distances can also be supplemented by additional angular information} are calculated and combined to the $i$th input vector of the NN in step 2. The idea behind this representation is the \emph{Embedded Atom Concept} stating that the energy $E_i$ only depends on the distance between to neighboring atoms.
	\item The vectors calculated in step 1 are now input in a fully connected NN consisting of 5 hidden layers with 240, 120, 60, 30, 10 units respectively from the innermost to the outermost layer. It uses the hyperbolic tangent as an activation function for all hidden layers and the identity activation for the output of the NN from the last hidden layer. The weights and biases are adapted by the Adam method \cite{kingma2017adam} with an exponentially decaying learning rate, which is a variant of stochastic gradient descent, applied to the parameterised loss functions:
	$$\mathcal L (p_\epsilon, p_f, p_\xi) = p_\epsilon \Delta \epsilon^2 + \frac{p_f}{3N} \sum_i |\Delta \vec F_i|^2 + \frac{p_\xi}{9} \norm{\Delta \Xi}^2$$
	$\Delta$ indicates the difference between the prediction and the training data, $N$ is the number of atoms, $\epsilon$ is the energy per atom, $\vec F_i$ is the force on atom $i$ and $\xi$ is the virial tensor $\Xi = - \frac{1}{2} \sum_i \vec R_i \otimes \vec F_i$. The parameters $p_\epsilon$ and $p_\xi$ are steadily increased during training whereas $p_f$ is decreased. Including the forces and the virial tensor in the loss function serves as a regularization of the network to prevent overfitting and to reduce the training time. The denominators in the fractions normalise the respective contributions based on the number of terms in each part.
\end{enumerate}
The main advantage of the approach by L. Zhang et al. is its accuracy without sacrificing  scalability. In the case of water the DeePMD method achieves similar performance to the original AIMD simulations and for molecules the results were slightly better than the GDML benchmark. Since the physical quantities are all sums of local contributions, the computational costs increase only linearly with the number of atoms and the calculations can be easily parallelised. This allows for accurate MD simulations on larger timescales and with more atoms. Additionally, the model benefits from not requiring any auxiliary quantities as opposed to other machine learning approaches like \cite{Chmiela_2017} or \cite{PhysRevLett.98.146401}, which simplifies the scheme.

One drawback of this local decomposition is the emergence of discontinuities in the outputs from the restriction to balls of a fixed radius. Furthermore,
%long-range Coulomb interactions are not treated explicitly, which may be necessary for some applications.  
the model's ability to represent long-range interactions is in question. These contribute only implicitly to the energy from interactions within the cutoff distance. S. Yue et al. \cite{article} investigate the impact this shortcoming and conclude that it strongly affects for example vapor phases properties, such as densities and second virial coefficients. Recent efforts to include long-range interactions comprise among others a decomposition of energy and forces into electrostatic and non-electrostatic contributions \cite{article}. However, this approach results in 2x higher computational costs. Thus, the inclusion of long-range interactions without sacrificing efficiency is still topic on ongoing research. 