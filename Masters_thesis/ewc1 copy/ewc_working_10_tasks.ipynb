{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "considered-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from argparse import ArgumentParser\n",
    "import numpy as np\n",
    "import torch\n",
    "from data import get_dataset, DATASET_CONFIGS\n",
    "from train import train\n",
    "from model import MLP\n",
    "import utils\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "compound-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_number = 10\n",
    "hidden_size1=512\n",
    "hidden_size2 = 256\n",
    "hidden_dropout_prob=0.5\n",
    "input_dropout_prob=0.2\n",
    "lamda=1000\n",
    "\n",
    "epochs_per_task = 100\n",
    "batch_size = 128\n",
    "test_size= 8192\n",
    "fisher_estimation_sample_size = 2048\n",
    "lr=1.e-3\n",
    "weight_decay = 0\n",
    "eval_log_interval = 250\n",
    "loss_log_interval= 250\n",
    "cuda=False\n",
    "\n",
    "# decide whether to use cuda or not.\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "# generate permutations for the tasks.\n",
    "np.random.seed(0)\n",
    "permutations = [\n",
    "    np.random.permutation(DATASET_CONFIGS['mnist']['size']**2) for\n",
    "    _ in range(task_number)\n",
    "]\n",
    "\n",
    "# prepare mnist datasets.\n",
    "train_datasets = [\n",
    "    get_dataset('mnist', permutation=p) for p in permutations\n",
    "]\n",
    "test_datasets = [\n",
    "    get_dataset('mnist', train=False, permutation=p) for p in permutations\n",
    "]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "subject-anime",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_plot(x):\n",
    "    for t, v in x.items():\n",
    "        plt.plot(list(range((t-1) * epochs_per_task, t * epochs_per_task)), v)\n",
    "\n",
    "def accuracy_plot(x):\n",
    "    for t, v in x.items():\n",
    "        plt.plot(list(range((t-1) * epochs_per_task, task_number * epochs_per_task)), v)\n",
    "    plt.ylim(0.8, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "certain-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_precision_plot(precisions, labels = []): #precisions needs to be in the form of the return value of train\n",
    "    for num, precision in enumerate(precisions):    \n",
    "        avg_precisions = []\n",
    "        total_epochs = task_number*epochs_per_task\n",
    "        for epoch in range (total_epochs):\n",
    "            avg_precision = 0\n",
    "            tasks_considered = epoch // epochs_per_task +1 #gives 1 for first task, 2 for second,...\n",
    "            for i in range(1,tasks_considered+1): #\n",
    "                avg_precision += precision[i][epoch - (i-1)*epochs_per_task]\n",
    "            avg_precision/=tasks_considered\n",
    "            avg_precisions.append(avg_precision)\n",
    "        plt.ylim(0.88, 0.94)\n",
    "        if (len(labels) == len(precisions)):\n",
    "            plt.plot(range(total_epochs), avg_precisions, label = labels[num])\n",
    "        else:\n",
    "            plt.plot(range(total_epochs), avg_precisions)\n",
    "    plt.legend()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "portuguese-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the model.\n",
    "mlp_no_dropout = MLP(\n",
    "    DATASET_CONFIGS['mnist']['size']**2,\n",
    "    DATASET_CONFIGS['mnist']['classes'],\n",
    "    hidden_size1,\n",
    "    hidden_size2,\n",
    "    0,\n",
    "    0,\n",
    "    lamda,\n",
    ")\n",
    "\n",
    "# initialize the parameters.\n",
    "#utils.gaussian_initialize(mlp_no_dropout)\n",
    "\n",
    "# prepare the cuda if needed.\n",
    "if cuda:\n",
    "    mlp_no_dropout.cuda()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "raised-volume",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=> task: 1/10 | epoch: 1/100 | progress: [1024/60000] (2%) | prec: 0.1094 | loss => ce: -0.01833 / ewc: 0.0 / total: -0.01833: : 8it [00:40,  5.12s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3292b8de6266>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# run the standard experiment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconsolidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m standard_prec_no_dropout, standard_total_loss_no_dropout, standard_ce_loss_no_dropout, standard_ewc_loss_no_dropout = train(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmlp_no_dropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_datasets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs_per_task\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/University-Coding/Masters_thesis/ewc1 copy/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_datasets, test_datasets, epochs_per_task, batch_size, test_size, consolidate, fisher_estimation_sample_size, lr, weight_decay, loss_log_interval, eval_log_interval, cuda)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mewc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mewc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mce_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mewc_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run the standard experiment.\n",
    "consolidate = False\n",
    "standard_prec_no_dropout, standard_total_loss_no_dropout, standard_ce_loss_no_dropout, standard_ewc_loss_no_dropout = train(\n",
    "    mlp_no_dropout, train_datasets, test_datasets,\n",
    "    epochs_per_task,\n",
    "    batch_size,\n",
    "    test_size,\n",
    "    consolidate,\n",
    "    fisher_estimation_sample_size,\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    eval_log_interval,\n",
    "    loss_log_interval,\n",
    "    cuda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot(standard_total_loss_no_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_plot(standard_prec_no_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_precision_plot([standard_prec_no_dropout])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda = 400\n",
    "mlp_consolidation_no_dropout = MLP(\n",
    "    DATASET_CONFIGS['mnist']['size']**2,\n",
    "    DATASET_CONFIGS['mnist']['classes'],\n",
    "    hidden_size,\n",
    "    hidden_layer_num,\n",
    "    0,\n",
    "    0,\n",
    "    lamda,\n",
    ")\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "utils.gaussian_initialize(mlp_consolidation_no_dropout)\n",
    "\n",
    "# run the standard experiment.\n",
    "consolidate = True\n",
    "ewc_prec_no_dropout, ewc_total_loss_no_dropout, ewc_ce_loss_no_dropout, ewc_ewc_loss_no_dropout =train(\n",
    "    mlp_consolidation_no_dropout, train_datasets, test_datasets,\n",
    "    epochs_per_task,\n",
    "    batch_size,\n",
    "    test_size,\n",
    "    consolidate,\n",
    "    fisher_estimation_sample_size,\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    eval_log_interval,\n",
    "    loss_log_interval,\n",
    "    cuda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot(ewc_total_loss_no_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_plot(ewc_prec_no_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-bidder",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_precision_plot([ewc_prec_no_dropout])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the model.\n",
    "mlp_dropout = MLP(\n",
    "    DATASET_CONFIGS['mnist']['size']**2,\n",
    "    DATASET_CONFIGS['mnist']['classes'],\n",
    "    hidden_size,\n",
    "    hidden_layer_num,\n",
    "    hidden_dropout_prob,\n",
    "    input_dropout_prob,\n",
    "    lamda,\n",
    ")\n",
    "#different to xavier intialisation ??? test\n",
    "# initialize the parameters.\n",
    "utils.gaussian_initialize(mlp_dropout)\n",
    "\n",
    "# prepare the cuda if needed.\n",
    "if cuda:\n",
    "    mlp.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the standard experiment.\n",
    "consolidate = False\n",
    "standard_prec_dropout, standard_total_loss_dropout, standard_ce_loss_dropout, standard_ewc_loss_dropout = train(\n",
    "    mlp_dropout, train_datasets, test_datasets,\n",
    "    epochs_per_task,\n",
    "    batch_size,\n",
    "    test_size,\n",
    "    consolidate,\n",
    "    fisher_estimation_sample_size,\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    eval_log_interval,\n",
    "    loss_log_interval,\n",
    "    cuda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot(standard_total_loss_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_plot(standard_prec_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_precision_plot([standard_prec_dropout])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-malaysia",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_consolidation_dropout = MLP(\n",
    "    DATASET_CONFIGS['mnist']['size']**2,\n",
    "    DATASET_CONFIGS['mnist']['classes'],\n",
    "    hidden_size,\n",
    "    hidden_layer_num,\n",
    "    hidden_dropout_prob,\n",
    "    input_dropout_prob,\n",
    "    lamda,\n",
    ")\n",
    "\n",
    "utils.gaussian_initialize(mlp_consolidation_dropout)\n",
    "\n",
    "# run the standard experiment.\n",
    "consolidate = True\n",
    "ewc_prec_dropout, ewc_total_loss_dropout, ewc_ce_loss_dropout, ewc_ewc_loss_dropout =train(\n",
    "    mlp_consolidation_dropout, train_datasets, test_datasets,\n",
    "    epochs_per_task,\n",
    "    batch_size,\n",
    "    test_size,\n",
    "    consolidate,\n",
    "    fisher_estimation_sample_size,\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    eval_log_interval,\n",
    "    loss_log_interval,\n",
    "    cuda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot(ewc_total_loss_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-fifth",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_plot(ewc_prec_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_precision_plot([ewc_prec_dropout])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_precision_plot([standard_prec_no_dropout,ewc_prec, standard_prec,  ewc_prec_dropout], [\"no dropout\", \"dropout\", \"ewc_40\", \"ewc_100\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-hello",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''lamda = 1000\n",
    "mlp_consolidation_1000 = MLP(\n",
    "    DATASET_CONFIGS['mnist']['size']**2,\n",
    "    DATASET_CONFIGS['mnist']['classes'],\n",
    "    hidden_size,\n",
    "    hidden_layer_num,\n",
    "    hidden_dropout_prob,\n",
    "    input_dropout_prob,\n",
    "    lamda,\n",
    ")\n",
    "\n",
    "# run the standard experiment.\n",
    "consolidate = True\n",
    "ewc_prec_1000, ewc_total_loss_1000, ewc_ce_loss_1000, ewc_ewc_loss_1000 =train(\n",
    "    mlp_consolidation_1000, train_datasets, test_datasets,\n",
    "    epochs_per_task,\n",
    "    batch_size,\n",
    "    test_size,\n",
    "    consolidate,\n",
    "    fisher_estimation_sample_size,\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    eval_log_interval,\n",
    "    loss_log_interval,\n",
    "    cuda\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_plot(ewc_total_loss_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy_plot(ewc_prec_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''lamda = 10\n",
    "mlp_consolidation_10 = MLP(\n",
    "    DATASET_CONFIGS['mnist']['size']**2,\n",
    "    DATASET_CONFIGS['mnist']['classes'],\n",
    "    hidden_size,\n",
    "    hidden_layer_num,\n",
    "    hidden_dropout_prob,\n",
    "    input_dropout_prob,\n",
    "    lamda,\n",
    ")\n",
    "\n",
    "# run the standard experiment.\n",
    "consolidate = True\n",
    "ewc_prec_10, ewc_total_loss_10, ewc_ce_loss_10, ewc_ewc_loss_10 =train(\n",
    "    mlp_consolidation_10, train_datasets, test_datasets,\n",
    "    epochs_per_task,\n",
    "    batch_size,\n",
    "    test_size,\n",
    "    consolidate,\n",
    "    fisher_estimation_sample_size,\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    eval_log_interval,\n",
    "    loss_log_interval,\n",
    "    cuda\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_plot(ewc_total_loss_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy_plot(ewc_prec_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''lamda = 80\n",
    "mlp_consolidation_80 = MLP(\n",
    "    DATASET_CONFIGS['mnist']['size']**2,\n",
    "    DATASET_CONFIGS['mnist']['classes'],\n",
    "    hidden_size,\n",
    "    hidden_layer_num,\n",
    "    hidden_dropout_prob,\n",
    "    input_dropout_prob,\n",
    "    lamda,\n",
    ")\n",
    "\n",
    "# run the standard experiment.\n",
    "consolidate = True\n",
    "ewc_prec_80, ewc_total_loss_80, ewc_ce_loss_80, ewc_ewc_loss_80 =train(\n",
    "    mlp_consolidation_80, train_datasets, test_datasets,\n",
    "    epochs_per_task,\n",
    "    batch_size,\n",
    "    test_size,\n",
    "    consolidate,\n",
    "    fisher_estimation_sample_size,\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    eval_log_interval,\n",
    "    loss_log_interval,\n",
    "    cuda\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-plant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_plot(ewc_total_loss_80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy_plot(ewc_prec_80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-malaysia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
