{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acting-marble",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from argparse import ArgumentParser\n",
    "import numpy as np\n",
    "import torch\n",
    "from data import PermutedMNIST\n",
    "from train import train\n",
    "from model import MLP\n",
    "import utils\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "fisher_estimation_sample_size = 2048\n",
    "weight_decay = 0\n",
    "cuda=False\n",
    "task_number = 5\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "advised-slovenia",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'task_number' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-57ff829dd8f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_permute_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#unpermuted data:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-57ff829dd8f2>\u001b[0m in \u001b[0;36mget_permute_mnist\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         train_loader[i] = torch.utils.data.DataLoader(PermutedMNIST(train = True, permute_idx=idx),\n\u001b[1;32m     19\u001b[0m                                                       \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'task_number' is not defined"
     ]
    }
   ],
   "source": [
    "def imshow(img,labels):\n",
    "    fig = plt.figure()\n",
    "    for i in range(6):\n",
    "        plt.subplot(2,3,i+1)\n",
    "        plt.tight_layout()\n",
    "        plt.imshow(img[i].view(28,28), cmap='gray', interpolation='none')\n",
    "        plt.title(\"Ground Truth: {}\".format(labels[i]))\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "            \n",
    "            \n",
    "def get_permute_mnist(task_number):\n",
    "   \n",
    "    train_loader = {}\n",
    "    test_loader = {}\n",
    "    idx = list(range(28 * 28))\n",
    "    for i in range(task_number):\n",
    "        train_loader[i] = torch.utils.data.DataLoader(PermutedMNIST(train = True, permute_idx=idx),\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      num_workers=4)\n",
    "        test_loader[i] = torch.utils.data.DataLoader(PermutedMNIST(train = False, permute_idx= idx),\n",
    "                                                     batch_size=batch_size)\n",
    "        random.shuffle(idx)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "train_loader, test_loader = get_permute_mnist(task_number)\n",
    "\n",
    "#unpermuted data:\n",
    "examples_unpermuted = enumerate(train_loader[0])\n",
    "\n",
    "_, (example_data_unpermuted, example_targets_unpermuted) = next(examples_unpermuted)\n",
    "\n",
    "imshow(example_data_unpermuted[:6], example_targets_unpermuted[:6])\n",
    "\n",
    "#permuted data:\n",
    "examples_permuted = enumerate(train_loader[1])\n",
    "\n",
    "_, (example_data_permuted, example_targets_permuted) = next(examples_permuted)\n",
    "imshow(example_data_permuted[:6], example_targets_permuted[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-argument",
   "metadata": {},
   "outputs": [],
   "source": [
    "#could also cut last 5 items which are ignored by early stopping\n",
    "def loss_plot(x):\n",
    "    num_epochs = 0\n",
    "    for task in range(1, task_number+1):\n",
    "        plt.plot(range(num_epochs+1, num_epochs + 1 + len(x[task])), x[task] )\n",
    "        num_epochs+= len(x[task])\n",
    "        \n",
    "def accuracy_plot(x):\n",
    "    total_epochs = len(x[1])\n",
    "    for task in range(1, task_number + 1):\n",
    "        plt.plot(range(total_epochs+1 - len(x[task]), total_epochs+1), x[task] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-anger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_precision_plot(precisions, labels = []): #precisions needs to be in the form of the return value of train\n",
    "    for num, precision in enumerate(precisions):    \n",
    "        avg_precisions = []\n",
    "        total_epochs = task_number*epochs_per_task\n",
    "        for epoch in range (total_epochs):\n",
    "            avg_precision = 0\n",
    "            tasks_considered = epoch // epochs_per_task +1 #gives 1 for first task, 2 for second,...\n",
    "            for i in range(1,tasks_considered+1): #\n",
    "                avg_precision += precision[i][epoch - (i-1)*epochs_per_task]\n",
    "            avg_precision/=tasks_considered\n",
    "            avg_precisions.append(avg_precision)\n",
    "        plt.ylim(0.88, 0.94)\n",
    "        if (len(labels) == len(precisions)):\n",
    "            plt.plot(range(total_epochs), avg_precisions, label = labels[num])\n",
    "        else:\n",
    "            plt.plot(range(total_epochs), avg_precisions)\n",
    "    plt.legend()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#high learning rate, no dropout, no early stopping\n",
    "hidden_size1=512\n",
    "hidden_size2 = 256\n",
    "lamda=1000\n",
    "lr=5.e-2\n",
    "hidden_dropout_prob=0\n",
    "input_dropout_prob=0\n",
    "early_stopping = False\n",
    "consolidate = False\n",
    "epochs_per_task =50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the model.\n",
    "mlp_no_dropout_no_earlystopping = MLP( 28*28, 10,\n",
    "    hidden_size1,\n",
    "    hidden_size2,\n",
    "    hidden_dropout_prob,\n",
    "    input_dropout_prob,\n",
    "    lamda,\n",
    ")\n",
    "\n",
    "# initialize the weights.\n",
    "utils.gaussian_initialize(mlp_no_dropout_no_earlystopping)\n",
    "\n",
    "# prepare the cuda if needed.\n",
    "if cuda:\n",
    "    mlp_no_dropout.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-period",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the standard experiment.\n",
    "(standard_prec_no_dropout_no_earlystopping, \n",
    " standard_total_loss_no_dropout_no_earlystopping,\n",
    " standard_ce_loss_no_dropout_no_earlystopping,\n",
    " standard_ewc_loss_no_dropout_no_earlystopping) = train(\n",
    "    mlp_no_dropout_no_earlystopping, train_loader, test_loader,\n",
    "    epochs_per_task,\n",
    "    batch_size,\n",
    "    consolidate,\n",
    "    fisher_estimation_sample_size,\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    early_stopping,\n",
    "    cuda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-mongolia",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot(standard_total_loss_no_dropout_no_earlystopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_plot(standard_prec_no_dropout_no_earlystopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_consolidation_no_dropout_no_earlystopping = MLP( 28*28, 10,\n",
    "    hidden_size1,\n",
    "    hidden_size2,\n",
    "    hidden_dropout_prob,\n",
    "    input_dropout_prob,\n",
    "    lamda,\n",
    ")\n",
    "\n",
    "utils.gaussian_initialize(mlp_consolidation_no_dropout_no_earlystopping)\n",
    "\n",
    "# run the standard experiment.\n",
    "consolidate = True\n",
    "(ewc_prec_no_dropout_no_earlystopping, \n",
    " ewc_total_loss_no_dropout_no_earlystopping, \n",
    " ewc_ce_loss_no_dropout_no_earlystopping, \n",
    " ewc_ewc_loss_no_dropout_no_earlystopping) = train(\n",
    "    mlp_consolidation_no_dropout_no_earlystopping, train_loader, test_loader,\n",
    "    epochs_per_task,\n",
    "    batch_size,\n",
    "    consolidate,\n",
    "    fisher_estimation_sample_size,\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    early_stopping,\n",
    "    cuda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot(ewc_total_loss_no_dropout_no_earlystopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_plot(ewc_prec_no_dropout_no_earlystopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-biography",
   "metadata": {},
   "outputs": [],
   "source": [
    "#high learning rate, dropout, no early stopping\n",
    "hidden_dropout_prob = 0.5\n",
    "input_dropout_prob = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the model.\n",
    "mlp_dropout_no_earlystopping = MLP(28*28, 10,\n",
    "    hidden_size1,\n",
    "    hidden_size2,\n",
    "    hidden_dropout_prob,\n",
    "    input_dropout_prob,\n",
    "    lamda,\n",
    ")\n",
    "\n",
    "# initialize the weights.\n",
    "utils.gaussian_initialize(mlp_dropout_no_earlystopping)\n",
    "\n",
    "# run the standard experiment.\n",
    "consolidate = False\n",
    "(standard_prec_dropout_no_earlystopping,\n",
    " standard_total_loss_dropout_no_earlystopping,\n",
    " standard_ce_loss_dropout_no_earlystopping,\n",
    " standard_ewc_loss_dropout_no_earlystopping) = train(\n",
    "    mlp_dropout_no_earlystopping, train_loader, test_loader,\n",
    "    epochs_per_task,\n",
    "    batch_size,\n",
    "    consolidate,\n",
    "    fisher_estimation_sample_size,\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    early_stopping,\n",
    "    cuda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot(standard_total_loss_dropout_no_earlystopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-detective",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_plot(standard_prec_dropout_no_earlystopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-syria",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_consolidation_dropout_no_earlystopping = MLP(28*28, 10,\n",
    "    hidden_size1,\n",
    "    hidden_size2,\n",
    "    hidden_dropout_prob,\n",
    "    input_dropout_prob,\n",
    "    lamda,\n",
    ")\n",
    "\n",
    "utils.gaussian_initialize(mlp_consolidation_dropout_no_earlystopping)\n",
    "\n",
    "# run the standard experiment.\n",
    "consolidate = True\n",
    "(ewc_prec_dropout_no_earlystopping, \n",
    " ewc_total_loss_dropout_no_earlystopping, \n",
    " ewc_ce_loss_dropout_no_earlystopping, \n",
    " ewc_ewc_loss_dropout_no_earlystopping) =train(\n",
    "    mlp_consolidation_dropout_no_earlystopping, train_loader, test_loader,\n",
    "    epochs_per_task,\n",
    "    batch_size,\n",
    "    consolidate,\n",
    "    fisher_estimation_sample_size,\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    early_stopping,\n",
    "    cuda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-suicide",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot(ewc_total_loss_dropout_no_earlystopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-logistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_plot(ewc_prec_dropout_no_earlystopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropout and early stopping\n",
    "early_stopping = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_dropout_earlystopping = MLP(28*28, 10,\n",
    "    hidden_size1,\n",
    "    hidden_size2,\n",
    "    hidden_dropout_prob,\n",
    "    input_dropout_prob,\n",
    "    lamda,\n",
    ")\n",
    "\n",
    "utils.gaussian_initialize(mlp_dropout_earlystopping)\n",
    "\n",
    "# run the standard experiment.\n",
    "consolidate = False\n",
    "(standard_prec_dropout_earlystopping, \n",
    " standard_total_loss_dropout_earlystopping,\n",
    " standard_ce_loss_dropout_earlystopping,\n",
    " standard_ewc_loss_dropout_earlystopping) = train(\n",
    "    mlp_dropout_earlystopping, train_loader, test_loader,\n",
    "    epochs_per_task,\n",
    "    batch_size,\n",
    "    consolidate,\n",
    "    fisher_estimation_sample_size,\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    early_stopping,\n",
    "    cuda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot(standard_total_loss_dropout_earlystopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_plot(standard_prec_dropout_earlystopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-sarah",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_consolidation_dropout_earlystopping = MLP(28*28, 10,\n",
    "    hidden_size1,\n",
    "    hidden_size2,\n",
    "    hidden_dropout_prob,\n",
    "    input_dropout_prob,\n",
    "    lamda,\n",
    ")\n",
    "\n",
    "utils.gaussian_initialize(mlp_consolidation_dropout_earlystopping)\n",
    "\n",
    "# run the standard experiment.\n",
    "consolidate = True\n",
    "(ewc_prec_dropout_earlystopping,\n",
    " ewc_total_loss_dropout_earlystopping, \n",
    " ewc_ce_loss_dropout_earlystopping,\n",
    " ewc_ewc_loss_dropout_earlystopping) =train(\n",
    "    mlp_consolidation_dropout_earlystopping, train_loader, test_loader,\n",
    "    epochs_per_task,\n",
    "    batch_size,\n",
    "    consolidate,\n",
    "    fisher_estimation_sample_size,\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    early_stopping,\n",
    "    cuda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot(ewc_total_loss_dropout_earlystopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-location",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_plot(ewc_prec_dropout_earlystopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-router",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropout, early stopping, wider layers, higher lambda\n",
    "lr = 1.e-2\n",
    "hidden_size1 = 1600\n",
    "hidden_size2 = 1400\n",
    "lamda = 2000\n",
    "task_number = 10\n",
    "epochs_per_task = 75\n",
    "train_loader, test_loader = get_permute_mnist(task_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-sleeve",
   "metadata": {},
   "outputs": [],
   "source": [
    "#does even higher lambda + greater network size help to get better long term results + more epochs\n",
    "mlp_optimised = MLP(28*28, 10,\n",
    "    hidden_size1,\n",
    "    hidden_size2,\n",
    "    hidden_dropout_prob,\n",
    "    input_dropout_prob,\n",
    "    lamda,\n",
    ")\n",
    "\n",
    "utils.gaussian_initialize(mlp_optimised)\n",
    "\n",
    "# run the standard experiment.\n",
    "consolidate = False\n",
    "(standard_prec_optimised,\n",
    " standard_total_loss_optimised,\n",
    " standard_ce_loss_optimised, \n",
    " standard_ewc_loss_optimised) =train(\n",
    "    mlp_optimised, train_loader, test_loader,\n",
    "    epochs_per_task,\n",
    "    batch_size,\n",
    "    consolidate,\n",
    "    fisher_estimation_sample_size,\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    early_stopping,\n",
    "    cuda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot(standard_total_loss_optimised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_plot(standard_prec_optimised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-correction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#does even higher lambda + greater network size help to get better long term results + more epochs\n",
    "mlp_consolidation_optimised = MLP(28*28, 10,\n",
    "    hidden_size1,\n",
    "    hidden_size2,\n",
    "    hidden_dropout_prob,\n",
    "    input_dropout_prob,\n",
    "    lamda,\n",
    ")\n",
    "\n",
    "utils.gaussian_initialize(mlp_consolidation_optimised)\n",
    "\n",
    "# run the standard experiment.\n",
    "consolidate = True\n",
    "(ewc_prec_optimised, \n",
    " ewc_total_loss_optimised,\n",
    " ewc_ce_loss_optimised, \n",
    " ewc_ewc_loss_optimised) =train(\n",
    "    mlp_consolidation_optimised, train_loader, test_loader,\n",
    "    epochs_per_task,\n",
    "    batch_size,\n",
    "    consolidate,\n",
    "    fisher_estimation_sample_size,\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    early_stopping,\n",
    "    cuda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot(ewc_total_loss_optimised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_plot(ewc_prec_optimised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare performance on first task:\n",
    "fig = plt.figure(figsize = (20,10))\n",
    "plt.ylim(0.65,1)\n",
    "plt.plot(range(1, len(standard_prec_no_dropout_no_earlystopping)+1),\n",
    "         standard_prec_no_dropout_no_earlystopping, label = 'std_no_drop_no_stop')\n",
    "plt.plot(range(1, len(ewc_prec_no_dropout_no_earlystopping)+1),\n",
    "         ewc_prec_no_dropout_no_earlystopping, label = 'ewc_no_drop_no_stop')\n",
    "plt.plot(range(1, len(standard_prec_dropout_no_earlystopping)+1),\n",
    "         standard_prec_dropout_no_earlystopping, label = 'std_drop_no_stop')\n",
    "plt.plot(range(1, len(ewc_prec_dropout_no_earlystopping)+1),\n",
    "         ewc_prec_dropout_no_earlystopping, label = 'ewc_drop_no_stop')\n",
    "plt.plot(range(1, len(standard_prec_dropopout_earlystopping)+1),\n",
    "         standard_prec_dropout_earlystopping, label = 'std_drop_stop')\n",
    "plt.plot(range(1, len(ewc_prec_dropout_earlystopping)+1),\n",
    "         ewc_prec_dropout_earlystopping, label = 'ewc_drop_stop')\n",
    "plt.plot(range(1, len(standard_prec_optimised)+1),\n",
    "         standard_prec_optimised, label = 'std_opt')\n",
    "plt.plot(range(1, len(ewc_prec_optimised)+1),\n",
    "         ewc_prec_optimised, label = 'ewc_opt')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda = 3000\n",
    "lr = 1.e-2\n",
    "#does even higher lambda + greater network size help to get better long term results + more epochs\n",
    "mlp_consolidation_optimised2 = MLP(28*28, 10,\n",
    "    hidden_size1,\n",
    "    hidden_size2,\n",
    "    hidden_dropout_prob,\n",
    "    input_dropout_prob,\n",
    "    lamda,\n",
    ")\n",
    "\n",
    "utils.gaussian_initialize(mlp_consolidation_optimised2)\n",
    "\n",
    "# run the standard experiment.\n",
    "consolidate = True\n",
    "(ewc_prec_optimised2, \n",
    " ewc_total_loss_optimised2,\n",
    " ewc_ce_loss_optimised2, \n",
    " ewc_ewc_loss_optimised2) =train(\n",
    "    mlp_consolidation_optimised2, train_loader, test_loader,\n",
    "    epochs_per_task,\n",
    "    batch_size,\n",
    "    consolidate,\n",
    "    fisher_estimation_sample_size,\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    early_stopping,\n",
    "    cuda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot(ewc_total_loss_optimised2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_plot(ewc_prec_optimised2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-musical",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lamda = 400\n",
    "lr = 5.e-2\n",
    "#does even higher lambda + greater network size help to get better long term results + more epochs\n",
    "mlp_consolidation_optimised2 = MLP(28*28, 10,\n",
    "    hidden_size1,\n",
    "    hidden_size2,\n",
    "    hidden_dropout_prob,\n",
    "    input_dropout_prob,\n",
    "    lamda,\n",
    ")\n",
    "\n",
    "utils.gaussian_initialize(mlp_consolidation_optimised2)\n",
    "\n",
    "# run the standard experiment.\n",
    "consolidate = True\n",
    "(ewc_prec_optimised2, \n",
    " ewc_total_loss_optimised2,\n",
    " ewc_ce_loss_optimised2, \n",
    " ewc_ewc_loss_optimised2) =train(\n",
    "    mlp_consolidation_optimised2, train_loader, test_loader,\n",
    "    epochs_per_task,\n",
    "    batch_size,\n",
    "    consolidate,\n",
    "    fisher_estimation_sample_size,\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    early_stopping,\n",
    "    cuda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-produce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_plot(ewc_total_loss_optimised2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-peninsula",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_plot(ewc_prec_optimised2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-bosnia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
